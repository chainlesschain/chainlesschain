# Whisper Local Server éƒ¨ç½²æŒ‡å—

æœ¬åœ° Whisper è¯­éŸ³è¯†åˆ«æœåŠ¡å™¨ï¼Œæä¾›ä¸ OpenAI API å…¼å®¹çš„æ¥å£ã€‚

---

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### æœ€ä½é…ç½®
- **CPU**: 4æ ¸å¿ƒä»¥ä¸Š
- **å†…å­˜**: 8GB RAM
- **å­˜å‚¨**: 5GB å¯ç”¨ç©ºé—´
- **Python**: 3.8+

### æ¨èé…ç½®
- **GPU**: NVIDIA GPU (CUDA æ”¯æŒ)
- **å†…å­˜**: 16GB+ RAM
- **å­˜å‚¨**: 10GB+ å¯ç”¨ç©ºé—´

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd backend/whisper-local-server

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. å¯åŠ¨æœåŠ¡å™¨

```bash
# æ–¹æ³• 1: ç›´æ¥è¿è¡Œ
python whisper_local_server.py

# æ–¹æ³• 2: ä½¿ç”¨ uvicorn
uvicorn whisper_local_server:app --host 0.0.0.0 --port 8000 --reload

# æ–¹æ³• 3: åå°è¿è¡Œ
nohup python whisper_local_server.py > whisper.log 2>&1 &
```

### 3. éªŒè¯æœåŠ¡

```bash
# æ£€æŸ¥å¥åº·çŠ¶æ€
curl http://localhost:8000/health

# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://localhost:8000/v1/models
```

---

## ğŸ“¦ æ¨¡å‹è¯´æ˜

### å¯ç”¨æ¨¡å‹

| æ¨¡å‹ | å‚æ•°é‡ | å†…å­˜å ç”¨ | é€Ÿåº¦ | å‡†ç¡®åº¦ |
|------|--------|----------|------|--------|
| tiny | 39M | ~1GB | æœ€å¿« | è¾ƒä½ |
| base | 74M | ~1GB | å¿« | ä¸­ç­‰ |
| small | 244M | ~2GB | ä¸­ç­‰ | è‰¯å¥½ |
| medium | 769M | ~5GB | æ…¢ | å¾ˆå¥½ |
| large | 1550M | ~10GB | æœ€æ…¢ | æœ€å¥½ |

### é¦–æ¬¡ä½¿ç”¨

é¦–æ¬¡ä½¿ç”¨æ—¶ï¼ŒWhisper ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ° `~/.cache/whisper/`ã€‚

**æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹**:
```python
import whisper
whisper.load_model("base")  # ä¸‹è½½ base æ¨¡å‹
```

---

## ğŸ”§ é…ç½®é€‰é¡¹

### ç¯å¢ƒå˜é‡

```bash
# è®¾ç½®è®¾å¤‡ï¼ˆcpu/cudaï¼‰
export WHISPER_DEVICE=cuda

# è®¾ç½®é»˜è®¤æ¨¡å‹
export WHISPER_DEFAULT_MODEL=base

# è®¾ç½®ç«¯å£
export WHISPER_PORT=8000
```

### æœåŠ¡å™¨é…ç½®

ç¼–è¾‘ `whisper_local_server.py`:

```python
# ä¿®æ”¹é»˜è®¤ç«¯å£
uvicorn.run(app, host="0.0.0.0", port=8000)

# ä¿®æ”¹é»˜è®¤æ¨¡å‹
load_model("small")  # æ”¹ä¸º small æ¨¡å‹

# ä¿®æ”¹è®¾å¤‡
device = "cpu"  # å¼ºåˆ¶ä½¿ç”¨ CPU
```

---

## ğŸ“¡ API ä½¿ç”¨

### è½¬å½•éŸ³é¢‘

```bash
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.mp3" \
  -F "model=base" \
  -F "language=zh"
```

### ç¿»è¯‘éŸ³é¢‘ï¼ˆç¿»è¯‘ä¸ºè‹±æ–‡ï¼‰

```bash
curl -X POST http://localhost:8000/v1/audio/translations \
  -F "file=@audio.mp3" \
  -F "model=base"
```

### åœ¨æ¡Œé¢åº”ç”¨ä¸­ä½¿ç”¨

æ¡Œé¢åº”ç”¨ä¼šè‡ªåŠ¨ä½¿ç”¨æœ¬åœ°æœåŠ¡å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼š

```javascript
// åœ¨è®¾ç½®ä¸­é…ç½®
{
  "speech": {
    "engine": "whisper-local",  // ä½¿ç”¨æœ¬åœ°æœåŠ¡
    "serverUrl": "http://localhost:8000",
    "modelSize": "base"
  }
}
```

---

## ğŸ³ Docker éƒ¨ç½²

### ä½¿ç”¨ Docker

```bash
# æ„å»ºé•œåƒ
docker build -t whisper-local-server .

# è¿è¡Œå®¹å™¨ï¼ˆCPUï¼‰
docker run -d -p 8000:8000 whisper-local-server

# è¿è¡Œå®¹å™¨ï¼ˆGPUï¼‰
docker run -d --gpus all -p 8000:8000 whisper-local-server
```

### Docker Compose

```yaml
version: '3.8'

services:
  whisper:
    build: ./backend/whisper-local-server
    ports:
      - "8000:8000"
    environment:
      - WHISPER_DEVICE=cuda
      - WHISPER_DEFAULT_MODEL=base
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

---

## ğŸ” æ€§èƒ½ä¼˜åŒ–

### 1. ä½¿ç”¨ GPU

ç¡®ä¿å®‰è£…äº† CUDA å’Œ PyTorch GPU ç‰ˆæœ¬ï¼š

```bash
# å®‰è£… PyTorch GPU ç‰ˆæœ¬
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 2. é¢„åŠ è½½æ¨¡å‹

åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹ï¼š

```python
# åœ¨ __main__ ä¸­æ·»åŠ 
load_model("base")
load_model("small")
```

### 3. è°ƒæ•´å¹¶å‘æ•°

ä½¿ç”¨ Gunicorn æé«˜å¹¶å‘å¤„ç†èƒ½åŠ›ï¼š

```bash
pip install gunicorn

gunicorn whisper_local_server:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000
```

### 4. ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹

å¯¹äºå®æ—¶åº”ç”¨ï¼Œä½¿ç”¨ `tiny` æˆ– `base` æ¨¡å‹ï¼š

```python
# é»˜è®¤ä½¿ç”¨ tiny æ¨¡å‹
load_model("tiny")
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: æ¨¡å‹ä¸‹è½½å¤±è´¥

**ç—‡çŠ¶**: é¦–æ¬¡è¿è¡Œæ—¶å¡ä½æˆ–è¶…æ—¶

**è§£å†³**:
```bash
# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
python -c "import whisper; whisper.load_model('base')"

# æˆ–è®¾ç½®ä»£ç†
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port
```

### é—®é¢˜ 2: CUDA ä¸å¯ç”¨

**ç—‡çŠ¶**: æ˜¾ç¤º "ä½¿ç”¨è®¾å¤‡: cpu" ä½†æœ‰ GPU

**è§£å†³**:
```bash
# æ£€æŸ¥ CUDA å®‰è£…
nvidia-smi

# é‡æ–°å®‰è£… PyTorch GPU ç‰ˆæœ¬
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### é—®é¢˜ 3: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: è½¬å½•å¤§æ–‡ä»¶æ—¶å´©æºƒ

**è§£å†³**:
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆtiny/baseï¼‰
- å¢åŠ ç³»ç»Ÿå†…å­˜
- åˆ†æ®µå¤„ç†éŸ³é¢‘æ–‡ä»¶

### é—®é¢˜ 4: ç«¯å£è¢«å ç”¨

**ç—‡çŠ¶**: "Address already in use"

**è§£å†³**:
```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
# Windows:
netstat -ano | findstr :8000

# Linux/Mac:
lsof -i :8000

# æ›´æ”¹ç«¯å£
uvicorn whisper_local_server:app --port 8001
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### æŸ¥çœ‹æ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f whisper.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep ERROR whisper.log
```

### æ€§èƒ½ç›‘æ§

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# æŸ¥çœ‹ CPU å’Œå†…å­˜
htop
```

---

## ğŸ”’ å®‰å…¨å»ºè®®

### 1. é™åˆ¶è®¿é—®

```python
# åªå…è®¸æœ¬åœ°è®¿é—®
uvicorn.run(app, host="127.0.0.1", port=8000)
```

### 2. æ·»åŠ è®¤è¯

```python
from fastapi import Header, HTTPException

async def verify_token(x_token: str = Header(...)):
    if x_token != "your-secret-token":
        raise HTTPException(status_code=401, detail="Invalid token")

@app.post("/v1/audio/transcriptions", dependencies=[Depends(verify_token)])
async def transcribe_audio(...):
    ...
```

### 3. é™åˆ¶æ–‡ä»¶å¤§å°

```python
from fastapi import UploadFile, File

@app.post("/v1/audio/transcriptions")
async def transcribe_audio(
    file: UploadFile = File(..., max_length=25 * 1024 * 1024)  # 25MB
):
    ...
```

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ
- CPU: Intel i7-10700K
- GPU: NVIDIA RTX 3080
- éŸ³é¢‘: 1åˆ†é’Ÿ MP3 æ–‡ä»¶

### ç»“æœ

| æ¨¡å‹ | CPU æ—¶é—´ | GPU æ—¶é—´ | å‡†ç¡®åº¦ |
|------|----------|----------|--------|
| tiny | 15s | 3s | 85% |
| base | 30s | 5s | 90% |
| small | 60s | 10s | 93% |
| medium | 120s | 20s | 95% |
| large | 240s | 40s | 97% |

---

## ğŸ”„ æ›´æ–°å’Œç»´æŠ¤

### æ›´æ–°ä¾èµ–

```bash
pip install --upgrade -r requirements.txt
```

### æ¸…ç†ç¼“å­˜

```bash
# æ¸…ç†æ¨¡å‹ç¼“å­˜
rm -rf ~/.cache/whisper/

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf /tmp/whisper_*
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [OpenAI Whisper](https://github.com/openai/whisper)
- [FastAPI æ–‡æ¡£](https://fastapi.tiangolo.com/)
- [PyTorch æ–‡æ¡£](https://pytorch.org/docs/)

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä½¿ç”¨ GPU**: æ˜¾è‘—æå‡æ€§èƒ½ï¼ˆ5-10å€ï¼‰
2. **é¢„åŠ è½½æ¨¡å‹**: å‡å°‘é¦–æ¬¡è¯·æ±‚å»¶è¿Ÿ
3. **é€‰æ‹©åˆé€‚çš„æ¨¡å‹**: å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®åº¦
4. **ç›‘æ§èµ„æº**: é¿å…å†…å­˜æº¢å‡º
5. **æ—¥å¿—è®°å½•**: ä¾¿äºé—®é¢˜æ’æŸ¥

---

**æœ€åæ›´æ–°**: 2026-01-09
**ç‰ˆæœ¬**: 1.0.0
