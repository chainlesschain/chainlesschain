#!/usr/bin/env node
/**
 * Cowork LLM Integration Script
 *
 * Purpose: Test and verify Ollama LLM integration with Cowork
 * - Check Ollama service availability
 * - Test LLM client functionality
 * - Verify Cowork LLM integration
 * - Benchmark performance
 */

const http = require('http');

console.log('ü§ñ Cowork LLM Integration\n');
console.log('='.repeat(60));

const stats = {
  checks: 0,
  passed: 0,
  failed: 0,
  warnings: 0,
};

function pass(message) {
  console.log(`‚úÖ ${message}`);
  stats.passed++;
  stats.checks++;
}

function fail(message) {
  console.log(`‚ùå ${message}`);
  stats.failed++;
  stats.checks++;
}

function warn(message) {
  console.log(`‚ö†Ô∏è  ${message}`);
  stats.warnings++;
}

function info(message) {
  console.log(`‚ÑπÔ∏è  ${message}`);
}

/**
 * Make HTTP request to Ollama
 */
function ollamaRequest(path, options = {}) {
  return new Promise((resolve, reject) => {
    const reqOptions = {
      hostname: 'localhost',
      port: 11434,
      path: path,
      method: options.method || 'GET',
      headers: {
        'Content-Type': 'application/json',
        ...options.headers,
      },
      timeout: 10000,
    };

    const req = http.request(reqOptions, (res) => {
      let data = '';

      res.on('data', (chunk) => {
        data += chunk;
      });

      res.on('end', () => {
        try {
          if (res.statusCode >= 200 && res.statusCode < 300) {
            resolve(JSON.parse(data));
          } else {
            reject(new Error(`HTTP ${res.statusCode}: ${data}`));
          }
        } catch (error) {
          reject(error);
        }
      });
    });

    req.on('error', (error) => {
      reject(error);
    });

    req.on('timeout', () => {
      req.destroy();
      reject(new Error('Request timeout'));
    });

    if (options.body) {
      req.write(JSON.stringify(options.body));
    }

    req.end();
  });
}

/**
 * Step 1: Check Ollama service
 */
async function checkOllamaService() {
  console.log('\nüìã Step 1: Checking Ollama Service\n');

  try {
    // Get available models
    const response = await ollamaRequest('/api/tags');

    if (response.models && response.models.length > 0) {
      pass(`Ollama service running (${response.models.length} models available)`);

      // List models
      info('Available models:');
      response.models.forEach((model) => {
        const sizeGB = (model.size / 1024 / 1024 / 1024).toFixed(2);
        console.log(`   - ${model.name} (${sizeGB}GB, ${model.details.parameter_size})`);
      });

      // Check for recommended models
      const recommendedModels = ['qwen2.5:latest', 'deepseek-r1:7b', 'llama3.1:latest'];
      const availableNames = response.models.map((m) => m.name);

      const hasRecommended = recommendedModels.some((rm) =>
        availableNames.some((an) => an.includes(rm.split(':')[0]))
      );

      if (hasRecommended) {
        pass('Recommended models available for Cowork');
      } else {
        warn('Consider downloading qwen2.5 or deepseek-r1 for better performance');
      }

      return response.models;
    } else {
      fail('No models available');
      warn('Download a model: ollama pull qwen2.5');
      return [];
    }
  } catch (error) {
    fail(`Ollama service unavailable: ${error.message}`);
    warn('Start Ollama: docker-compose up -d ollama');
    warn('Or install locally from: https://ollama.ai');
    return [];
  }
}

/**
 * Step 2: Test LLM generation
 */
async function testLLMGeneration(models) {
  console.log('\nüìã Step 2: Testing LLM Generation\n');

  if (models.length === 0) {
    warn('Skipping LLM tests (no models available)');
    return false;
  }

  // Use first available model
  const model = models[0].name;
  info(`Testing with model: ${model}`);

  const testPrompt = 'Say "Hello, Cowork!" in exactly 3 words.';

  try {
    const startTime = Date.now();

    const response = await ollamaRequest('/api/generate', {
      method: 'POST',
      body: {
        model: model,
        prompt: testPrompt,
        stream: false,
        options: {
          temperature: 0.1,
          num_predict: 20,
        },
      },
    });

    const duration = Date.now() - startTime;

    if (response.response) {
      pass(`LLM generation successful (${duration}ms)`);
      info(`Response: "${response.response.trim()}"`);

      // Check performance
      if (duration < 2000) {
        pass('Response time < 2s (excellent)');
      } else if (duration < 5000) {
        pass('Response time < 5s (good)');
      } else {
        warn(`Response time ${duration}ms (consider GPU acceleration)`);
      }

      return true;
    } else {
      fail('LLM generation failed (no response)');
      return false;
    }
  } catch (error) {
    fail(`LLM generation error: ${error.message}`);
    return false;
  }
}

/**
 * Step 3: Test task analysis
 */
async function testTaskAnalysis(models) {
  console.log('\nüìã Step 3: Testing Task Analysis\n');

  if (models.length === 0) {
    warn('Skipping task analysis test');
    return;
  }

  const model = models[0].name;
  const taskAnalysisPrompt = `Analyze the following task and provide recommendations:

**Task Description**: Review authentication module for security vulnerabilities
**Task Type**: code-review
**Priority**: High

Please analyze this task and provide:
1. **Complexity Level** (1-10): How complex is this task?
2. **Estimated Time**: Approximate time to complete (in minutes)
3. **Required Skills**: What skills/capabilities are needed?
4. **Recommended Agents**: How many agents should work on this? (1 for simple, multiple for complex)
5. **Parallelization**: Can this task be parallelized? (Yes/No)
6. **Risk Level** (Low/Medium/High): Potential risks or challenges

Format your response as JSON:
{
  "complexity": <number>,
  "estimatedTime": <number>,
  "requiredSkills": ["skill1", "skill2"],
  "recommendedAgents": <number>,
  "canParallelize": <boolean>,
  "riskLevel": "Low|Medium|High",
  "reasoning": "Brief explanation"
}`;

  try {
    info('Analyzing code review task...');
    const startTime = Date.now();

    const response = await ollamaRequest('/api/generate', {
      method: 'POST',
      body: {
        model: model,
        prompt: taskAnalysisPrompt,
        stream: false,
        options: {
          temperature: 0.7,
          num_predict: 500,
        },
      },
    });

    const duration = Date.now() - startTime;

    if (response.response) {
      // Try to parse JSON from response
      const jsonMatch = response.response.match(/\{[\s\S]*\}/);

      if (jsonMatch) {
        try {
          const analysis = JSON.parse(jsonMatch[0]);
          pass(`Task analysis successful (${duration}ms)`);

          info('Analysis results:');
          console.log(`   - Complexity: ${analysis.complexity}/10`);
          console.log(`   - Estimated Time: ${analysis.estimatedTime} minutes`);
          console.log(`   - Required Skills: ${analysis.requiredSkills?.join(', ') || 'N/A'}`);
          console.log(`   - Recommended Agents: ${analysis.recommendedAgents}`);
          console.log(`   - Can Parallelize: ${analysis.canParallelize}`);
          console.log(`   - Risk Level: ${analysis.riskLevel}`);

          if (analysis.complexity && analysis.estimatedTime && analysis.riskLevel) {
            pass('LLM provided complete analysis');
          } else {
            warn('Analysis incomplete (missing some fields)');
          }
        } catch (parseError) {
          warn('LLM response not in valid JSON format');
          info(`Raw response: ${response.response.substring(0, 200)}...`);
        }
      } else {
        warn('No JSON found in LLM response');
      }
    }
  } catch (error) {
    fail(`Task analysis failed: ${error.message}`);
  }
}

/**
 * Step 4: Verify Cowork integration files
 */
function verifyCoworkIntegration() {
  console.log('\nüìã Step 4: Verifying Cowork LLM Integration\n');

  const fs = require('fs');
  const path = require('path');

  const integrationPath = path.join(
    __dirname,
    '../src/main/cowork/integrations/llm-integration.js'
  );

  if (!fs.existsSync(integrationPath)) {
    fail('Cowork LLM integration file not found');
    return false;
  }

  const content = fs.readFileSync(integrationPath, 'utf-8');

  // Check for required methods
  const requiredMethods = [
    'analyzeTask',
    'recommendAgent',
    'decomposeTask',
    'resolveConflict',
    'generateStrategy',
  ];

  requiredMethods.forEach((method) => {
    if (content.includes(method)) {
      pass(`Method ${method}() exists`);
    } else {
      fail(`Method ${method}() not found`);
    }
  });

  // Check for fallback methods
  if (content.includes('_fallbackTaskAnalysis')) {
    pass('Fallback methods implemented (works offline)');
  }

  // Check default model
  if (content.includes("'qwen2:7b'")) {
    warn('Default model is qwen2:7b (consider updating to qwen2.5)');
  }

  return true;
}

/**
 * Step 5: Performance benchmarks
 */
async function performanceBenchmarks(models) {
  console.log('\nüìã Step 5: Performance Benchmarks\n');

  if (models.length === 0) {
    warn('Skipping benchmarks (no models available)');
    return;
  }

  const model = models[0].name;
  const testCases = [
    { name: 'Simple Query', tokens: 20 },
    { name: 'Task Analysis', tokens: 500 },
    { name: 'Complex Strategy', tokens: 1000 },
  ];

  info(`Benchmarking model: ${model}\n`);

  const results = [];

  for (const testCase of testCases) {
    try {
      const startTime = Date.now();

      await ollamaRequest('/api/generate', {
        method: 'POST',
        body: {
          model: model,
          prompt: 'Test prompt for benchmarking',
          stream: false,
          options: {
            num_predict: testCase.tokens,
          },
        },
      });

      const duration = Date.now() - startTime;
      const tokensPerSec = (testCase.tokens / (duration / 1000)).toFixed(1);

      results.push({
        name: testCase.name,
        tokens: testCase.tokens,
        duration,
        tokensPerSec,
      });

      console.log(`   ${testCase.name}:`);
      console.log(`     Tokens: ${testCase.tokens}, Duration: ${duration}ms`);
      console.log(`     Speed: ${tokensPerSec} tokens/sec`);
    } catch (error) {
      warn(`Benchmark failed for ${testCase.name}: ${error.message}`);
    }
  }

  if (results.length > 0) {
    const avgDuration = results.reduce((sum, r) => sum + r.duration, 0) / results.length;
    info(`\n   Average response time: ${avgDuration.toFixed(0)}ms`);

    if (avgDuration < 2000) {
      pass('Excellent performance (<2s average)');
    } else if (avgDuration < 5000) {
      pass('Good performance (<5s average)');
    } else {
      warn('Consider GPU acceleration for better performance');
    }
  }
}

/**
 * Step 6: Create configuration guide
 */
function createConfigurationGuide(models) {
  console.log('\nüìã Step 6: Creating LLM Configuration Guide\n');

  const fs = require('fs');
  const path = require('path');

  const modelList = models.map((m) => {
    const sizeGB = (m.size / 1024 / 1024 / 1024).toFixed(2);
    return `- **${m.name}** (${sizeGB}GB, ${m.details.parameter_size})`;
  }).join('\n');

  const guide = `# Cowork LLM Integration Guide

## Ollama Service Status

‚úÖ **Service Running**: http://localhost:11434
üì¶ **Available Models**: ${models.length}

${modelList}

## Cowork LLM Integration

**File**: \`src/main/cowork/integrations/llm-integration.js\`

### Core Capabilities

1. **Task Analysis** (\`analyzeTask()\`)
   - Analyzes task complexity (1-10 scale)
   - Estimates completion time
   - Identifies required skills
   - Recommends agent count
   - Assesses parallelization potential
   - Evaluates risk level

2. **Agent Recommendation** (\`recommendAgent()\`)
   - Matches task requirements with agent capabilities
   - Considers current workload
   - Provides confidence score
   - Explains selection reasoning

3. **Task Decomposition** (\`decomposeTask()\`)
   - Breaks complex tasks into 3-7 subtasks
   - Identifies dependencies
   - Estimates time for each subtask
   - Maps required skills

4. **Conflict Resolution** (\`resolveConflict()\`)
   - Analyzes conflicting agent opinions
   - Weighs arguments and context
   - Provides clear decision
   - Explains rationale with confidence score

5. **Strategy Generation** (\`generateStrategy()\`)
   - Creates execution plans (sequential/parallel/hybrid)
   - Allocates resources optimally
   - Identifies risks
   - Defines success criteria

### Configuration

**Default Settings**:
\`\`\`javascript
{
  model: 'qwen2:7b',      // LLM model to use
  temperature: 0.7,       // Creativity (0.0-1.0)
  maxTokens: 1000        // Max response length
}
\`\`\`

**Recommended Models for Cowork**:
- **qwen2.5:latest** - Best balance of speed and quality
- **deepseek-r1:7b** - Excellent reasoning capabilities
- **llama3.1:latest** - Strong general performance

### Usage Examples

#### Example 1: Analyze Task

\`\`\`javascript
const llmIntegration = new CoworkLLMIntegration(llmService);

const task = {
  description: "Review authentication module for security vulnerabilities",
  type: "code-review",
  priority: "high"
};

const analysis = await llmIntegration.analyzeTask(task);
// Returns: {
//   complexity: 8,
//   estimatedTime: 45,
//   requiredSkills: ["security", "code-review"],
//   recommendedAgents: 2,
//   canParallelize: true,
//   riskLevel: "High",
//   reasoning: "Complex security audit requires expertise"
// }
\`\`\`

#### Example 2: Recommend Agent

\`\`\`javascript
const recommendation = await llmIntegration.recommendAgent({
  task: codeReviewTask,
  availableAgents: [
    { id: "agent-1", name: "Security Reviewer", capabilities: ["security", "owasp"] },
    { id: "agent-2", name: "Code Auditor", capabilities: ["code-quality", "performance"] }
  ]
});
// Returns: {
//   agentId: "agent-1",
//   confidence: 0.92,
//   reasoning: "Agent has required security expertise"
// }
\`\`\`

#### Example 3: Decompose Complex Task

\`\`\`javascript
const subtasks = await llmIntegration.decomposeTask({
  description: "Implement user authentication system",
  type: "feature-development"
});
// Returns: [
//   { description: "Design authentication schema", estimatedTime: 30, ... },
//   { description: "Implement password hashing", estimatedTime: 45, ... },
//   { description: "Add session management", estimatedTime: 60, ... }
// ]
\`\`\`

## Performance Expectations

### Response Times (qwen2.5:latest on CPU)

| Task Type | Tokens | Expected Time |
|-----------|--------|---------------|
| Simple Query | 20 | <1s |
| Task Analysis | 500 | 2-5s |
| Task Decomposition | 1000 | 5-10s |
| Strategy Generation | 1500 | 8-15s |

**GPU Acceleration**: 3-5x faster with NVIDIA GPU + CUDA

### Fallback Behavior

If LLM is unavailable, Cowork uses **heuristic fallbacks**:
- Task analysis: Based on description length
- Agent recommendation: First available agent
- Conflict resolution: Majority vote
- Decomposition: Returns empty (manual intervention needed)

## Model Selection Guide

### Small Tasks (< 5 min)
- **qwen2.5:latest** - Fast, accurate
- **deepseek-r1:7b** - Good reasoning

### Complex Tasks (> 15 min)
- **deepseek-r1:14b** - Superior analysis
- **deepseek-v2:16b** - Advanced reasoning

### Embedding Tasks
- **bge-m3:latest** - Optimized for embeddings

## Troubleshooting

### Issue: LLM responses are slow

**Solutions**:
1. Use smaller model (qwen2.5 instead of deepseek-v2:16b)
2. Reduce \`maxTokens\` setting
3. Enable GPU acceleration (Docker with NVIDIA runtime)
4. Lower \`temperature\` for faster, more deterministic responses

### Issue: LLM responses are not in JSON format

**Solution**: LLM integration has robust parsing with fallbacks
- Extracts JSON using regex: \`/\\{[\\s\\S]*\\}/\`
- Falls back to heuristics if parsing fails

### Issue: Ollama service unavailable

**Check**:
\`\`\`bash
# Check if service is running
curl http://localhost:11434/api/tags

# Start Ollama (Docker)
docker-compose up -d ollama

# Or install locally
# https://ollama.ai
\`\`\`

### Issue: Out of memory

**Solutions**:
1. Use smaller model (7B instead of 14B)
2. Reduce context length
3. Increase Docker memory limit (8GB recommended)

## Updating Model

### Change Default Model

Edit \`src/main/cowork/integrations/llm-integration.js\`:

\`\`\`javascript
this.model = 'qwen2.5:latest';  // Update this line
\`\`\`

### Download New Model

\`\`\`bash
# Via Docker
docker exec chainlesschain-ollama ollama pull qwen2.5

# Or locally
ollama pull qwen2.5
\`\`\`

## Monitoring LLM Usage

### Enable Metrics

Cowork tracks LLM usage:
- Request count
- Average response time
- Token consumption
- Error rate
- Fallback usage

Access via Cowork Analytics dashboard:
\`http://localhost:5173/#/cowork/analytics\`

## Next Steps

1. ‚úÖ Ollama service verified
2. ‚úÖ LLM integration tested
3. üìù Update default model to qwen2.5 (optional)
4. üß™ Test with real Cowork tasks
5. üìä Monitor performance metrics

---

**Integration Status**: ‚úÖ READY
**Recommended Next**: Create team templates (Task #5)

---

*Generated: ${new Date().toISOString().split('T')[0]}*
*Ollama Service: Running*
*Available Models: ${models.length}*
`;

  const guidePath = path.join(__dirname, '..', '.cowork', 'llm-integration-guide.md');
  fs.writeFileSync(guidePath, guide, 'utf-8');

  pass(`LLM integration guide created: ${guidePath}`);
  info('Guide includes:');
  console.log('   - Available models and capabilities');
  console.log('   - Usage examples for all 5 methods');
  console.log('   - Performance expectations');
  console.log('   - Troubleshooting tips');
}

/**
 * Generate summary report
 */
function generateReport(models) {
  console.log('\n' + '='.repeat(60));
  console.log('üìä Cowork LLM Integration Summary');
  console.log('='.repeat(60));

  console.log(`\n‚úÖ Checks Completed: ${stats.checks}`);
  console.log(`‚úÖ Passed: ${stats.passed}`);
  console.log(`‚ùå Failed: ${stats.failed}`);
  console.log(`‚ö†Ô∏è  Warnings: ${stats.warnings}`);

  const successRate = stats.checks > 0
    ? Math.round((stats.passed / stats.checks) * 100)
    : 0;

  console.log(`\nüìà Success Rate: ${successRate}%`);

  console.log('\nüí° Ollama Service:');
  console.log(`   Status: ${models.length > 0 ? '‚úÖ Running' : '‚ùå Unavailable'}`);
  console.log(`   Available Models: ${models.length}`);
  console.log(`   Endpoint: http://localhost:11434`);

  console.log('\nü§ñ Cowork LLM Integration:');
  console.log('   File: src/main/cowork/integrations/llm-integration.js');
  console.log('   Methods: analyzeTask, recommendAgent, decomposeTask,');
  console.log('            resolveConflict, generateStrategy');
  console.log('   Fallback: ‚úÖ Heuristic methods available');

  if (stats.failed === 0) {
    console.log('\n‚ú® LLM integration ready! Cowork agents can use AI decision-making.\n');

    console.log('üöÄ Next Steps:');
    console.log('   1. LLM integration is verified');
    console.log('   2. Cowork agents can call LLM for task analysis');
    console.log('   3. Start desktop app to use LLM-powered features');
    console.log('   4. View guide: .cowork/llm-integration-guide.md');
  } else {
    console.log(`\n‚ö†Ô∏è  Integration completed with ${stats.failed} issue(s).\n`);
  }

  console.log('\nüìç Configuration Guide:');
  console.log('   Location: .cowork/llm-integration-guide.md');
  console.log('   Includes: Examples, troubleshooting, model selection\n');

  return stats.failed === 0 ? 0 : 1;
}

/**
 * Main execution
 */
async function main() {
  try {
    const models = await checkOllamaService();

    if (models.length > 0) {
      await testLLMGeneration(models);
      await testTaskAnalysis(models);
      await performanceBenchmarks(models);
    }

    verifyCoworkIntegration();
    createConfigurationGuide(models);

    const exitCode = generateReport(models);
    process.exit(exitCode);
  } catch (error) {
    console.error('\n‚ùå Integration failed:', error.message);
    console.error(error.stack);
    process.exit(1);
  }
}

// Run script
if (require.main === module) {
  main();
}

module.exports = { main, stats };
