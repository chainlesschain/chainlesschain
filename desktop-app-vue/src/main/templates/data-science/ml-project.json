{
  "id": "tpl_data_ml_002",
  "name": "ml_project_template",
  "display_name": "æœºå™¨å­¦ä¹ é¡¹ç›®",
  "description": "å¿«é€Ÿæ­å»ºæœºå™¨å­¦ä¹ é¡¹ç›®ï¼ŒåŒ…å«æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œéƒ¨ç½²",
  "icon": "ğŸ¤–",
  "category": "data-science",
  "subcategory": "machine-learning",
  "tags": [
    "æœºå™¨å­¦ä¹ ",
    "AI",
    "æ¨¡å‹è®­ç»ƒ",
    "æ·±åº¦å­¦ä¹ "
  ],
  "project_type": "app",
  "prompt_template": "è¯·å¸®æˆ‘ç”Ÿæˆæœºå™¨å­¦ä¹ é¡¹ç›®æ¡†æ¶ï¼Œè¦æ±‚:\n\n**é¡¹ç›®ä¿¡æ¯ï¼š**\n- é¡¹ç›®åç§°ï¼š{{projectName}}\n- é—®é¢˜ç±»å‹ï¼š{{problemType}}\n- ç®—æ³•é€‰æ‹©ï¼š{{algorithm}}\n- æ¡†æ¶ï¼š{{mlFramework}}\n\n**é¡¹ç›®ç»“æ„ï¼š**\n\n```\n{{projectName}}/\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ raw/              # åŸå§‹æ•°æ®\nâ”‚   â”œâ”€â”€ processed/        # å¤„ç†åæ•°æ®\nâ”‚   â””â”€â”€ external/         # å¤–éƒ¨æ•°æ®\nâ”‚\nâ”œâ”€â”€ notebooks/            # Jupyter notebooks\nâ”‚   â”œâ”€â”€ 01_EDA.ipynb\nâ”‚   â”œâ”€â”€ 02_Feature_Engineering.ipynb\nâ”‚   â”œâ”€â”€ 03_Model_Training.ipynb\nâ”‚   â””â”€â”€ 04_Model_Evaluation.ipynb\nâ”‚\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ data/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ load_data.py\nâ”‚   â”‚   â”œâ”€â”€ preprocess.py\nâ”‚   â”‚   â””â”€â”€ feature_engineering.py\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ models/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ train.py\nâ”‚   â”‚   â”œâ”€â”€ predict.py\nâ”‚   â”‚   â””â”€â”€ evaluate.py\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ utils/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ config.py\nâ”‚   â”‚   â””â”€â”€ logger.py\nâ”‚   â”‚\nâ”‚   â””â”€â”€ visualization/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â””â”€â”€ plots.py\nâ”‚\nâ”œâ”€â”€ models/               # ä¿å­˜çš„æ¨¡å‹\nâ”‚   â”œâ”€â”€ model_v1.pkl\nâ”‚   â””â”€â”€ best_model.pkl\nâ”‚\nâ”œâ”€â”€ tests/               # å•å…ƒæµ‹è¯•\nâ”‚   â”œâ”€â”€ test_data.py\nâ”‚   â””â”€â”€ test_model.py\nâ”‚\nâ”œâ”€â”€ config/\nâ”‚   â”œâ”€â”€ config.yaml\nâ”‚   â””â”€â”€ hyperparameters.yaml\nâ”‚\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ setup.py\nâ”œâ”€â”€ README.md\nâ””â”€â”€ .gitignore\n```\n\n**requirements.txtï¼š**\n\n```\n# æ ¸å¿ƒåº“\nnumpy==1.26.2\npandas==2.1.3\nscikit-learn==1.3.2\n\n{{#eq mlFramework \"TensorFlow/Keras\"}}\n# æ·±åº¦å­¦ä¹ \ntensorflow==2.15.0\nkeras==2.15.0\n{{/eq}}\n{{#eq mlFramework \"PyTorch\"}}\n# æ·±åº¦å­¦ä¹ \ntorch==2.1.0\ntorchvision==0.16.0\n{{/eq}}\n{{#eq mlFramework \"XGBoost\"}}\n# æ¢¯åº¦æå‡\nxgboost==2.0.2\nlightgbm==4.1.0\n{{/eq}}\n\n# æ•°æ®å¯è§†åŒ–\nmatplotlib==3.8.2\nseaborn==0.13.0\nplotly==5.18.0\n\n# æ¨¡å‹è§£é‡Š\nshap==0.43.0\neli5==0.13.0\n\n# å®éªŒè¿½è¸ª\nmlflow==2.9.0\nwandb==0.16.1\n\n# å·¥å…·\njupyter==1.0.0\ntqdm==4.66.1\npyyaml==6.0.1\njoblib==1.3.2\n\n# æµ‹è¯•\npytest==7.4.3\npytest-cov==4.1.0\n```\n\n**README.mdï¼š**\n\n```markdown\n# {{projectName}}\n\n{{projectDescription}}\n\n## é—®é¢˜å®šä¹‰\n\n**é—®é¢˜ç±»å‹**ï¼š{{problemType}}\n\n{{#eq problemType \"åˆ†ç±»\"}}\n**ç›®æ ‡**ï¼šå°†æ•°æ®åˆ†ä¸º{{numClasses}}ä¸ªç±»åˆ«\n**è¯„ä¼°æŒ‡æ ‡**ï¼šAccuracy, Precision, Recall, F1-Score, AUC-ROC\n{{/eq}}\n{{#eq problemType \"å›å½’\"}}\n**ç›®æ ‡**ï¼šé¢„æµ‹è¿ç»­å€¼\n**è¯„ä¼°æŒ‡æ ‡**ï¼šMAE, MSE, RMSE, RÂ²\n{{/eq}}\n{{#eq problemType \"èšç±»\"}}\n**ç›®æ ‡**ï¼šå‘ç°æ•°æ®ä¸­çš„è‡ªç„¶åˆ†ç»„\n**è¯„ä¼°æŒ‡æ ‡**ï¼šSilhouette Score, Davies-Bouldin Index\n{{/eq}}\n\n## æ•°æ®é›†\n\n- **æ•°æ®æ¥æº**ï¼š{{dataSource}}\n- **æ ·æœ¬æ•°é‡**ï¼š{{sampleSize}}\n- **ç‰¹å¾æ•°é‡**ï¼š{{featureCount}}\n- **ç›®æ ‡å˜é‡**ï¼š{{targetVariable}}\n\n## å¿«é€Ÿå¼€å§‹\n\n### å®‰è£…ä¾èµ–\n\n```bash\npip install -r requirements.txt\n```\n\n### æ•°æ®å‡†å¤‡\n\n```bash\n# å°†åŸå§‹æ•°æ®æ”¾å…¥ data/raw/\npython src/data/preprocess.py\n```\n\n### æ¨¡å‹è®­ç»ƒ\n\n```bash\npython src/models/train.py --config config/config.yaml\n```\n\n### æ¨¡å‹è¯„ä¼°\n\n```bash\npython src/models/evaluate.py --model models/best_model.pkl\n```\n\n### é¢„æµ‹\n\n```bash\npython src/models/predict.py --input data/test.csv --output predictions.csv\n```\n\n## é¡¹ç›®æµç¨‹\n\n### 1. æ•°æ®æ¢ç´¢ï¼ˆEDAï¼‰\n\nè¿è¡Œ `notebooks/01_EDA.ipynb`\n\n- æ•°æ®æ¦‚è§ˆ\n- ç¼ºå¤±å€¼åˆ†æ\n- ç‰¹å¾åˆ†å¸ƒ\n- ç›¸å…³æ€§åˆ†æ\n\n### 2. ç‰¹å¾å·¥ç¨‹\n\nè¿è¡Œ `notebooks/02_Feature_Engineering.ipynb`\n\n- ç‰¹å¾æ¸…æ´—\n- ç‰¹å¾è½¬æ¢\n- ç‰¹å¾é€‰æ‹©\n- ç‰¹å¾åˆ›å»º\n\n### 3. æ¨¡å‹è®­ç»ƒ\n\nè¿è¡Œ `notebooks/03_Model_Training.ipynb`\n\n- æ•°æ®åˆ’åˆ†\n- æ¨¡å‹é€‰æ‹©\n- è¶…å‚æ•°è°ƒä¼˜\n- äº¤å‰éªŒè¯\n\n### 4. æ¨¡å‹è¯„ä¼°\n\nè¿è¡Œ `notebooks/04_Model_Evaluation.ipynb`\n\n- æ€§èƒ½æŒ‡æ ‡\n- æ··æ·†çŸ©é˜µ\n- ç‰¹å¾é‡è¦æ€§\n- æ¨¡å‹è§£é‡Š\n\n## æ¨¡å‹æ€§èƒ½\n\n| æ¨¡å‹ | {{metric1}} | {{metric2}} | {{metric3}} |\n|------|-------------|-------------|-------------|\n| {{model1}} | {{score1_1}} | {{score1_2}} | {{score1_3}} |\n| {{model2}} | {{score2_1}} | {{score2_2}} | {{score2_3}} |\n| **æœ€ä¼˜æ¨¡å‹** | **{{bestScore1}}** | **{{bestScore2}}** | **{{bestScore3}}** |\n\n## éƒ¨ç½²\n\n### ä½¿ç”¨Flask API\n\n```bash\npython app.py\n```\n\nè®¿é—® http://localhost:5000/predict\n\n### ä½¿ç”¨Docker\n\n```bash\ndocker build -t {{projectName}} .\ndocker run -p 5000:5000 {{projectName}}\n```\n\n## è®¸å¯è¯\n\nMIT\n```\n\n**src/data/preprocess.pyï¼š**\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport yaml\n\nclass DataPreprocessor:\n    def __init__(self, config_path='config/config.yaml'):\n        with open(config_path, 'r') as f:\n            self.config = yaml.safe_load(f)\n        \n        self.scaler = StandardScaler()\n        self.label_encoder = LabelEncoder()\n    \n    def load_data(self, file_path):\n        \"\"\"åŠ è½½æ•°æ®\"\"\"\n        return pd.read_csv(file_path)\n    \n    def handle_missing_values(self, df):\n        \"\"\"å¤„ç†ç¼ºå¤±å€¼\"\"\"\n        # æ•°å€¼å‹ï¼šå¡«å……ä¸­ä½æ•°\n        numeric_cols = df.select_dtypes(include=[np.number]).columns\n        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n        \n        # ç±»åˆ«å‹ï¼šå¡«å……ä¼—æ•°\n        categorical_cols = df.select_dtypes(include=['object']).columns\n        df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n        \n        return df\n    \n    def encode_categorical(self, df, categorical_cols):\n        \"\"\"ç¼–ç ç±»åˆ«ç‰¹å¾\"\"\"\n        for col in categorical_cols:\n            df[col] = self.label_encoder.fit_transform(df[col])\n        return df\n    \n    def scale_features(self, X_train, X_test):\n        \"\"\"æ ‡å‡†åŒ–ç‰¹å¾\"\"\"\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        X_test_scaled = self.scaler.transform(X_test)\n        return X_train_scaled, X_test_scaled\n    \n    def preprocess(self, input_path, output_path):\n        \"\"\"å®Œæ•´é¢„å¤„ç†æµç¨‹\"\"\"\n        # åŠ è½½æ•°æ®\n        df = self.load_data(input_path)\n        \n        # å¤„ç†ç¼ºå¤±å€¼\n        df = self.handle_missing_values(df)\n        \n        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾\n        X = df.drop(self.config['target_column'], axis=1)\n        y = df[self.config['target_column']]\n        \n        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, \n            test_size=self.config['test_size'],\n            random_state=self.config['random_state']\n        )\n        \n        # ä¿å­˜å¤„ç†åçš„æ•°æ®\n        X_train.to_csv(f'{output_path}/X_train.csv', index=False)\n        X_test.to_csv(f'{output_path}/X_test.csv', index=False)\n        y_train.to_csv(f'{output_path}/y_train.csv', index=False)\n        y_test.to_csv(f'{output_path}/y_test.csv', index=False)\n        \n        print(f'æ•°æ®é¢„å¤„ç†å®Œæˆï¼')\n        print(f'è®­ç»ƒé›†å¤§å°: {X_train.shape}')\n        print(f'æµ‹è¯•é›†å¤§å°: {X_test.shape}')\n\nif __name__ == '__main__':\n    preprocessor = DataPreprocessor()\n    preprocessor.preprocess('data/raw/data.csv', 'data/processed')\n```\n\n**src/models/train.pyï¼š**\n\n```python\nimport pandas as pd\nimport numpy as np\nimport yaml\nimport joblib\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nimport mlflow\nimport mlflow.sklearn\n\nclass ModelTrainer:\n    def __init__(self, config_path='config/config.yaml'):\n        with open(config_path, 'r') as f:\n            self.config = yaml.safe_load(f)\n        \n        self.model = None\n    \n    def load_data(self):\n        \"\"\"åŠ è½½è®­ç»ƒæ•°æ®\"\"\"\n        X_train = pd.read_csv('data/processed/X_train.csv')\n        y_train = pd.read_csv('data/processed/y_train.csv').values.ravel()\n        return X_train, y_train\n    \n    def train_model(self, X_train, y_train):\n        \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n{{#eq problemType \"åˆ†ç±»\"}}\n{{#eq algorithm \"éšæœºæ£®æ—\"}}\n        self.model = RandomForestClassifier(\n            n_estimators=self.config['n_estimators'],\n            max_depth=self.config['max_depth'],\n            random_state=self.config['random_state'],\n            n_jobs=-1\n        )\n{{/eq}}\n{{#eq algorithm \"XGBoost\"}}\n        from xgboost import XGBClassifier\n        self.model = XGBClassifier(\n            n_estimators=self.config['n_estimators'],\n            max_depth=self.config['max_depth'],\n            learning_rate=self.config['learning_rate'],\n            random_state=self.config['random_state']\n        )\n{{/eq}}\n{{/eq}}\n{{#eq problemType \"å›å½’\"}}\n        from sklearn.ensemble import RandomForestRegressor\n        self.model = RandomForestRegressor(\n            n_estimators=self.config['n_estimators'],\n            max_depth=self.config['max_depth'],\n            random_state=self.config['random_state']\n        )\n{{/eq}}\n        \n        # è®­ç»ƒ\n        self.model.fit(X_train, y_train)\n        \n        # äº¤å‰éªŒè¯\n        cv_scores = cross_val_score(\n            self.model, X_train, y_train, \n            cv=5, scoring='{{scoring}}'\n        )\n        \n        print(f'äº¤å‰éªŒè¯å¾—åˆ†: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})')\n        \n        return self.model\n    \n    def hyperparameter_tuning(self, X_train, y_train):\n        \"\"\"è¶…å‚æ•°è°ƒä¼˜\"\"\"\n        param_grid = self.config['param_grid']\n        \n        grid_search = GridSearchCV(\n            self.model,\n            param_grid,\n            cv=5,\n            scoring='{{scoring}}',\n            n_jobs=-1,\n            verbose=2\n        )\n        \n        grid_search.fit(X_train, y_train)\n        \n        print(f'æœ€ä½³å‚æ•°: {grid_search.best_params_}')\n        print(f'æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}')\n        \n        self.model = grid_search.best_estimator_\n        return self.model\n    \n    def save_model(self, model_path='models/best_model.pkl'):\n        \"\"\"ä¿å­˜æ¨¡å‹\"\"\"\n        joblib.dump(self.model, model_path)\n        print(f'æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}')\n    \n    def log_to_mlflow(self, X_train, y_train):\n        \"\"\"è®°å½•åˆ°MLflow\"\"\"\n        with mlflow.start_run():\n            # è®°å½•å‚æ•°\n            mlflow.log_params(self.config)\n            \n            # è®­ç»ƒå¹¶è®°å½•æŒ‡æ ‡\n            cv_scores = cross_val_score(self.model, X_train, y_train, cv=5)\n            mlflow.log_metric('cv_mean_score', cv_scores.mean())\n            mlflow.log_metric('cv_std_score', cv_scores.std())\n            \n            # è®°å½•æ¨¡å‹\n            mlflow.sklearn.log_model(self.model, 'model')\n\nif __name__ == '__main__':\n    trainer = ModelTrainer()\n    X_train, y_train = trainer.load_data()\n    \n    # è®­ç»ƒæ¨¡å‹\n    model = trainer.train_model(X_train, y_train)\n    \n    # è¶…å‚æ•°è°ƒä¼˜ï¼ˆå¯é€‰ï¼‰\n    # model = trainer.hyperparameter_tuning(X_train, y_train)\n    \n    # ä¿å­˜æ¨¡å‹\n    trainer.save_model()\n    \n    # MLflowè¿½è¸ªï¼ˆå¯é€‰ï¼‰\n    # trainer.log_to_mlflow(X_train, y_train)\n```\n\n**src/models/evaluate.pyï¼š**\n\n```python\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nimport shap\n\nclass ModelEvaluator:\n    def __init__(self, model_path='models/best_model.pkl'):\n        self.model = joblib.load(model_path)\n    \n    def load_test_data(self):\n        \"\"\"åŠ è½½æµ‹è¯•æ•°æ®\"\"\"\n        X_test = pd.read_csv('data/processed/X_test.csv')\n        y_test = pd.read_csv('data/processed/y_test.csv').values.ravel()\n        return X_test, y_test\n    \n{{#eq problemType \"åˆ†ç±»\"}}\n    def evaluate_classification(self, X_test, y_test):\n        \"\"\"è¯„ä¼°åˆ†ç±»æ¨¡å‹\"\"\"\n        # é¢„æµ‹\n        y_pred = self.model.predict(X_test)\n        y_proba = self.model.predict_proba(X_test)\n        \n        # åˆ†ç±»æŠ¥å‘Š\n        print('åˆ†ç±»æŠ¥å‘Š:')\n        print(classification_report(y_test, y_pred))\n        \n        # æ··æ·†çŸ©é˜µ\n        cm = confusion_matrix(y_test, y_pred)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n        plt.title('æ··æ·†çŸ©é˜µ')\n        plt.ylabel('çœŸå®å€¼')\n        plt.xlabel('é¢„æµ‹å€¼')\n        plt.savefig('outputs/confusion_matrix.png')\n        \n        # ROCæ›²çº¿\n        if len(np.unique(y_test)) == 2:  # äºŒåˆ†ç±»\n            fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])\n            auc = roc_auc_score(y_test, y_proba[:, 1])\n            \n            plt.figure(figsize=(8, 6))\n            plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n            plt.plot([0, 1], [0, 1], 'k--')\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.title('ROC Curve')\n            plt.legend()\n            plt.savefig('outputs/roc_curve.png')\n{{/eq}}\n\n{{#eq problemType \"å›å½’\"}}\n    def evaluate_regression(self, X_test, y_test):\n        \"\"\"è¯„ä¼°å›å½’æ¨¡å‹\"\"\"\n        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n        \n        y_pred = self.model.predict(X_test)\n        \n        mae = mean_absolute_error(y_test, y_pred)\n        mse = mean_squared_error(y_test, y_pred)\n        rmse = np.sqrt(mse)\n        r2 = r2_score(y_test, y_pred)\n        \n        print(f'MAE: {mae:.4f}')\n        print(f'MSE: {mse:.4f}')\n        print(f'RMSE: {rmse:.4f}')\n        print(f'RÂ²: {r2:.4f}')\n        \n        # é¢„æµ‹å€¼vsçœŸå®å€¼\n        plt.figure(figsize=(8, 6))\n        plt.scatter(y_test, y_pred, alpha=0.5)\n        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n        plt.xlabel('çœŸå®å€¼')\n        plt.ylabel('é¢„æµ‹å€¼')\n        plt.title('é¢„æµ‹å€¼ vs çœŸå®å€¼')\n        plt.savefig('outputs/predictions.png')\n{{/eq}}\n    \n    def feature_importance(self, X_test, feature_names):\n        \"\"\"ç‰¹å¾é‡è¦æ€§åˆ†æ\"\"\"\n        if hasattr(self.model, 'feature_importances_'):\n            importances = self.model.feature_importances_\n            indices = np.argsort(importances)[::-1][:20]  # Top 20\n            \n            plt.figure(figsize=(10, 6))\n            plt.bar(range(len(indices)), importances[indices])\n            plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45, ha='right')\n            plt.title('Top 20 ç‰¹å¾é‡è¦æ€§')\n            plt.tight_layout()\n            plt.savefig('outputs/feature_importance.png')\n    \n    def shap_analysis(self, X_test):\n        \"\"\"SHAPå€¼åˆ†æ\"\"\"\n        explainer = shap.TreeExplainer(self.model)\n        shap_values = explainer.shap_values(X_test)\n        \n        # Summary plot\n        shap.summary_plot(shap_values, X_test, show=False)\n        plt.savefig('outputs/shap_summary.png', bbox_inches='tight')\n\nif __name__ == '__main__':\n    evaluator = ModelEvaluator()\n    X_test, y_test = evaluator.load_test_data()\n    \n{{#eq problemType \"åˆ†ç±»\"}}\n    evaluator.evaluate_classification(X_test, y_test)\n{{/eq}}\n{{#eq problemType \"å›å½’\"}}\n    evaluator.evaluate_regression(X_test, y_test)\n{{/eq}}\n    \n    evaluator.feature_importance(X_test, X_test.columns)\n    # evaluator.shap_analysis(X_test.sample(100))  # SHAPåˆ†æ\n```\n\n**config/config.yamlï¼š**\n\n```yaml\n# æ•°æ®é…ç½®\ntarget_column: '{{targetVariable}}'\ntest_size: 0.2\nrandom_state: 42\n\n# æ¨¡å‹å‚æ•°\nn_estimators: 100\nmax_depth: 10\nlearning_rate: 0.1\n\n# è¶…å‚æ•°æœç´¢ç©ºé—´\nparam_grid:\n  n_estimators: [50, 100, 200]\n  max_depth: [5, 10, 15, 20]\n  min_samples_split: [2, 5, 10]\n```\n\n**.gitignoreï¼š**\n\n```\n# æ•°æ®\ndata/raw/*.csv\ndata/processed/*.csv\n\n# æ¨¡å‹\nmodels/*.pkl\nmodels/*.h5\n\n# Python\n__pycache__/\n*.py[cod]\n*$py.class\n.Python\nvenv/\nENV/\n\n# Jupyter\n.ipynb_checkpoints/\n\n# MLflow\nmlruns/\n\n# è¾“å‡º\noutputs/\n*.png\n*.jpg\n\n# æ—¥å¿—\n*.log\n```\n\n**æœ€ä½³å®è·µå»ºè®®ï¼š**\n\n1. **æ•°æ®ç‰ˆæœ¬æ§åˆ¶**\n   - ä½¿ç”¨DVCç®¡ç†æ•°æ®ç‰ˆæœ¬\n   - è®°å½•æ•°æ®æ¥æºå’Œå¤„ç†è¿‡ç¨‹\n\n2. **å®éªŒè¿½è¸ª**\n   - ä½¿ç”¨MLflowæˆ–W&Bè¿½è¸ªå®éªŒ\n   - è®°å½•æ‰€æœ‰è¶…å‚æ•°å’ŒæŒ‡æ ‡\n\n3. **ä»£ç è´¨é‡**\n   - ç¼–å†™å•å…ƒæµ‹è¯•\n   - ä½¿ç”¨ç±»å‹æç¤º\n   - éµå¾ªPEP 8è§„èŒƒ\n\n4. **æ¨¡å‹å¯è§£é‡Šæ€§**\n   - ä½¿ç”¨SHAPåˆ†æ\n   - ç”Ÿæˆç‰¹å¾é‡è¦æ€§æŠ¥å‘Š\n   - å¯è§†åŒ–æ¨¡å‹å†³ç­–\n\n5. **æ¨¡å‹éƒ¨ç½²**\n   - å®¹å™¨åŒ–ï¼ˆDockerï¼‰\n   - APIåŒ–ï¼ˆFlask/FastAPIï¼‰\n   - ç›‘æ§æ¨¡å‹æ€§èƒ½æ¼‚ç§»",
  "variables_schema": [
    {
      "name": "projectName",
      "label": "é¡¹ç›®åç§°",
      "type": "string",
      "required": true,
      "placeholder": "ä¾‹å¦‚ï¼šcustomer-churn-prediction"
    },
    {
      "name": "projectDescription",
      "label": "é¡¹ç›®æè¿°",
      "type": "string",
      "required": false,
      "default": "æœºå™¨å­¦ä¹ é¡¹ç›®",
      "placeholder": "ç®€çŸ­æè¿°é¡¹ç›®ç›®æ ‡"
    },
    {
      "name": "problemType",
      "label": "é—®é¢˜ç±»å‹",
      "type": "select",
      "required": true,
      "options": [
        {
          "value": "åˆ†ç±»",
          "label": "åˆ†ç±»ï¼ˆClassificationï¼‰"
        },
        {
          "value": "å›å½’",
          "label": "å›å½’ï¼ˆRegressionï¼‰"
        },
        {
          "value": "èšç±»",
          "label": "èšç±»ï¼ˆClusteringï¼‰"
        }
      ]
    },
    {
      "name": "algorithm",
      "label": "ç®—æ³•é€‰æ‹©",
      "type": "select",
      "required": false,
      "default": "éšæœºæ£®æ—",
      "options": [
        {
          "value": "éšæœºæ£®æ—",
          "label": "éšæœºæ£®æ—"
        },
        {
          "value": "XGBoost",
          "label": "XGBoost"
        },
        {
          "value": "ç¥ç»ç½‘ç»œ",
          "label": "ç¥ç»ç½‘ç»œ"
        },
        {
          "value": "SVM",
          "label": "æ”¯æŒå‘é‡æœº"
        }
      ]
    },
    {
      "name": "mlFramework",
      "label": "MLæ¡†æ¶",
      "type": "select",
      "required": false,
      "default": "Scikit-learn",
      "options": [
        {
          "value": "Scikit-learn",
          "label": "Scikit-learn"
        },
        {
          "value": "TensorFlow/Keras",
          "label": "TensorFlow/Keras"
        },
        {
          "value": "PyTorch",
          "label": "PyTorch"
        },
        {
          "value": "XGBoost",
          "label": "XGBoost/LightGBM"
        }
      ]
    },
    {
      "name": "dataSource",
      "label": "æ•°æ®æ¥æº",
      "type": "string",
      "required": false,
      "default": "CSVæ–‡ä»¶",
      "placeholder": "ä¾‹å¦‚ï¼šKaggleã€ä¸šåŠ¡æ•°æ®åº“"
    },
    {
      "name": "targetVariable",
      "label": "ç›®æ ‡å˜é‡",
      "type": "string",
      "required": false,
      "default": "target",
      "placeholder": "ä¾‹å¦‚ï¼šchurn, price"
    },
    {
      "name": "sampleSize",
      "label": "æ ·æœ¬æ•°é‡",
      "type": "string",
      "required": false,
      "placeholder": "ä¾‹å¦‚ï¼š10000"
    },
    {
      "name": "featureCount",
      "label": "ç‰¹å¾æ•°é‡",
      "type": "string",
      "required": false,
      "placeholder": "ä¾‹å¦‚ï¼š50"
    }
  ],
  "file_structure": {
    "root": "{{projectName}}",
    "files": [
      {
        "path": "README.md",
        "type": "markdown"
      },
      {
        "path": "requirements.txt",
        "type": "document"
      },
      {
        "path": "config/config.yaml",
        "type": "document"
      },
      {
        "path": ".gitignore",
        "type": "document"
      }
    ]
  },
  "default_files": [],
  "is_builtin": true,
  "author": "ChainlessChain Team",
  "version": "1.0.0",
  "usage_count": 0,
  "rating": 0,
  "rating_count": 0,
  "required_skills": [
    "skill_data_science",
    "skill_data_analysis",
    "skill_code_development"
  ],
  "required_tools": [
    "tool_data_preprocessor",
    "tool_chart_generator",
    "tool_python_project_setup",
    "tool_ml_model_trainer",
    "tool_model_evaluator",
    "tool_feature_engineer"
  ],
  "execution_engine": "ml"
}