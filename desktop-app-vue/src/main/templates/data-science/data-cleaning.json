{
  "id": "tpl_data_science_cleaning_005",
  "name": "data_cleaning",
  "display_name": "æ•°æ®æ¸…æ´—å·¥ä½œæµ",
  "description": "ä¸“ä¸šçš„æ•°æ®æ¸…æ´—æ¨¡æ¿ï¼Œæ¶µç›–ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€é‡å¤æ•°æ®æ¸…é™¤ã€æ•°æ®ç±»å‹è½¬æ¢ã€æ•°æ®éªŒè¯ç­‰å®Œæ•´æµç¨‹",
  "icon": "ğŸ§¹",
  "category": "data-science",
  "subcategory": "æ•°æ®é¢„å¤„ç†",
  "tags": ["æ•°æ®æ¸…æ´—", "æ•°æ®è´¨é‡", "é¢„å¤„ç†", "ETL", "æ•°æ®éªŒè¯"],
  "project_type": "data",
  "prompt_template": "è¯·å¸®æˆ‘ç”Ÿæˆæ•°æ®æ¸…æ´—å·¥ä½œæµæ–‡æ¡£ï¼Œè¦æ±‚:\n\n# {{projectName}} - æ•°æ®æ¸…æ´—å·¥ä½œæµ\n\n## 1. é¡¹ç›®æ¦‚è¿°\n\n**æ•°æ®æº**: {{dataSource}}\n**æ•°æ®è§„æ¨¡**: {{dataScale}}\n**æ¸…æ´—ç›®æ ‡**: {{cleaningGoal}}\n**è´¨é‡æ ‡å‡†**: {{qualityStandard}}\n\n## 2. æ•°æ®è´¨é‡è¯„ä¼°\n\n### 2.1 åˆæ­¥æ£€æŸ¥\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# åŠ è½½æ•°æ®\ndf = pd.read_csv('{{dataSource}}')\n\n# åŸºæœ¬ä¿¡æ¯\nprint(\"æ•°æ®å½¢çŠ¶:\", df.shape)\nprint(\"\\næ•°æ®ç±»å‹:\")\nprint(df.dtypes)\nprint(\"\\nåŸºæœ¬ç»Ÿè®¡:\")\nprint(df.describe())\n\n# å‰å‡ è¡Œæ•°æ®\ndf.head(10)\n```\n\n### 2.2 æ•°æ®è´¨é‡é—®é¢˜è¯†åˆ«\n\n```python\nclass DataQualityChecker:\n    def __init__(self, df):\n        self.df = df\n        self.quality_report = {}\n    \n    def check_missing_values(self):\n        \"\"\"æ£€æŸ¥ç¼ºå¤±å€¼\"\"\"\n        missing = self.df.isnull().sum()\n        missing_pct = 100 * missing / len(self.df)\n        \n        missing_df = pd.DataFrame({\n            'åˆ—å': missing.index,\n            'ç¼ºå¤±æ•°é‡': missing.values,\n            'ç¼ºå¤±æ¯”ä¾‹(%)': missing_pct.values\n        })\n        \n        self.quality_report['missing'] = missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0]\n        return self.quality_report['missing']\n    \n    def check_duplicates(self):\n        \"\"\"æ£€æŸ¥é‡å¤è®°å½•\"\"\"\n        duplicates = self.df.duplicated()\n        dup_count = duplicates.sum()\n        dup_pct = 100 * dup_count / len(self.df)\n        \n        self.quality_report['duplicates'] = {\n            'é‡å¤è®°å½•æ•°': dup_count,\n            'é‡å¤æ¯”ä¾‹(%)': dup_pct,\n            'é‡å¤è¡Œç´¢å¼•': self.df[duplicates].index.tolist()\n        }\n        return self.quality_report['duplicates']\n    \n    def check_outliers(self, columns=None, method='iqr'):\n        \"\"\"æ£€æŸ¥å¼‚å¸¸å€¼\"\"\"\n        if columns is None:\n            columns = self.df.select_dtypes(include=[np.number]).columns\n        \n        outliers = {}\n        \n        for col in columns:\n            if method == 'iqr':\n                Q1 = self.df[col].quantile(0.25)\n                Q3 = self.df[col].quantile(0.75)\n                IQR = Q3 - Q1\n                \n                lower_bound = Q1 - 1.5 * IQR\n                upper_bound = Q3 + 1.5 * IQR\n                \n                outlier_mask = (self.df[col] < lower_bound) | (self.df[col] > upper_bound)\n                outlier_count = outlier_mask.sum()\n                \n                if outlier_count > 0:\n                    outliers[col] = {\n                        'å¼‚å¸¸å€¼æ•°é‡': outlier_count,\n                        'å¼‚å¸¸å€¼æ¯”ä¾‹(%)': 100 * outlier_count / len(self.df),\n                        'ä¸‹ç•Œ': lower_bound,\n                        'ä¸Šç•Œ': upper_bound,\n                        'å¼‚å¸¸å€¼': self.df[outlier_mask][col].tolist()[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª\n                    }\n            \n            elif method == 'zscore':\n                z_scores = np.abs((self.df[col] - self.df[col].mean()) / self.df[col].std())\n                outlier_mask = z_scores > 3\n                outlier_count = outlier_mask.sum()\n                \n                if outlier_count > 0:\n                    outliers[col] = {\n                        'å¼‚å¸¸å€¼æ•°é‡': outlier_count,\n                        'å¼‚å¸¸å€¼æ¯”ä¾‹(%)': 100 * outlier_count / len(self.df),\n                        'é˜ˆå€¼': 3,\n                        'å¼‚å¸¸å€¼': self.df[outlier_mask][col].tolist()[:10]\n                    }\n        \n        self.quality_report['outliers'] = outliers\n        return outliers\n    \n    def check_data_types(self):\n        \"\"\"æ£€æŸ¥æ•°æ®ç±»å‹æ˜¯å¦åˆç†\"\"\"\n        type_issues = []\n        \n        for col in self.df.columns:\n            # æ£€æŸ¥åº”è¯¥æ˜¯æ•°å€¼çš„åˆ—\n            if col.lower() in ['id', 'age', 'price', 'amount', 'count']:\n                if self.df[col].dtype == 'object':\n                    type_issues.append({\n                        'åˆ—å': col,\n                        'å½“å‰ç±»å‹': str(self.df[col].dtype),\n                        'å»ºè®®ç±»å‹': 'numeric',\n                        'é—®é¢˜': 'åº”ä¸ºæ•°å€¼ç±»å‹ä½†å½“å‰ä¸ºå¯¹è±¡ç±»å‹'\n                    })\n            \n            # æ£€æŸ¥åº”è¯¥æ˜¯æ—¥æœŸçš„åˆ—\n            if 'date' in col.lower() or 'time' in col.lower():\n                if self.df[col].dtype != 'datetime64[ns]':\n                    type_issues.append({\n                        'åˆ—å': col,\n                        'å½“å‰ç±»å‹': str(self.df[col].dtype),\n                        'å»ºè®®ç±»å‹': 'datetime',\n                        'é—®é¢˜': 'åº”ä¸ºæ—¥æœŸç±»å‹ä½†å½“å‰ä¸æ˜¯'\n                    })\n        \n        self.quality_report['type_issues'] = type_issues\n        return type_issues\n    \n    def check_invalid_values(self, rules):\n        \"\"\"æ£€æŸ¥ä¸åˆæ³•çš„å€¼\"\"\"\n        invalid = {}\n        \n        for col, rule in rules.items():\n            if col not in self.df.columns:\n                continue\n            \n            if 'range' in rule:\n                min_val, max_val = rule['range']\n                invalid_mask = (self.df[col] < min_val) | (self.df[col] > max_val)\n                invalid_count = invalid_mask.sum()\n                \n                if invalid_count > 0:\n                    invalid[col] = {\n                        'ä¸åˆæ³•æ•°é‡': invalid_count,\n                        'è§„åˆ™': f'åº”åœ¨ [{min_val}, {max_val}] èŒƒå›´å†…',\n                        'ç¤ºä¾‹': self.df[invalid_mask][col].head(5).tolist()\n                    }\n            \n            if 'allowed_values' in rule:\n                allowed = rule['allowed_values']\n                invalid_mask = ~self.df[col].isin(allowed)\n                invalid_count = invalid_mask.sum()\n                \n                if invalid_count > 0:\n                    invalid[col] = {\n                        'ä¸åˆæ³•æ•°é‡': invalid_count,\n                        'è§„åˆ™': f'åªå…è®¸: {allowed}',\n                        'å®é™…å€¼': self.df[invalid_mask][col].unique().tolist()[:10]\n                    }\n        \n        self.quality_report['invalid_values'] = invalid\n        return invalid\n    \n    def generate_report(self):\n        \"\"\"ç”Ÿæˆè´¨é‡æŠ¥å‘Š\"\"\"\n        print(\"=\"*50)\n        print(\"æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š\")\n        print(\"=\"*50)\n        \n        print(\"\\n1. ç¼ºå¤±å€¼:\")\n        print(self.quality_report.get('missing', 'æ— ç¼ºå¤±å€¼'))\n        \n        print(\"\\n2. é‡å¤è®°å½•:\")\n        print(self.quality_report.get('duplicates', 'æ— é‡å¤è®°å½•'))\n        \n        print(\"\\n3. å¼‚å¸¸å€¼:\")\n        if self.quality_report.get('outliers'):\n            for col, info in self.quality_report['outliers'].items():\n                print(f\"\\n  {col}: {info['å¼‚å¸¸å€¼æ•°é‡']}ä¸ª ({info['å¼‚å¸¸å€¼æ¯”ä¾‹(%)']:.2f}%)\")\n        else:\n            print(\"  æ— å¼‚å¸¸å€¼\")\n        \n        print(\"\\n4. æ•°æ®ç±»å‹é—®é¢˜:\")\n        if self.quality_report.get('type_issues'):\n            for issue in self.quality_report['type_issues']:\n                print(f\"  {issue}\")\n        else:\n            print(\"  æ— ç±»å‹é—®é¢˜\")\n        \n        return self.quality_report\n\n# ä½¿ç”¨ç¤ºä¾‹\nchecker = DataQualityChecker(df)\nchecker.check_missing_values()\nchecker.check_duplicates()\nchecker.check_outliers()\nchecker.check_data_types()\n\n# å®šä¹‰éªŒè¯è§„åˆ™\nvalidation_rules = {\n    'age': {'range': (0, 120)},\n    'price': {'range': (0, float('inf'))},\n    'status': {'allowed_values': ['active', 'inactive', 'pending']}\n}\nchecker.check_invalid_values(validation_rules)\n\n# ç”ŸæˆæŠ¥å‘Š\nreport = checker.generate_report()\n```\n\n## 3. æ•°æ®æ¸…æ´—æ“ä½œ\n\n### 3.1 ç¼ºå¤±å€¼å¤„ç†\n\n```python\nclass MissingValueHandler:\n    def __init__(self, df):\n        self.df = df.copy()\n    \n    def drop_missing(self, threshold=0.5, axis=0):\n        \"\"\"åˆ é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„è¡Œæˆ–åˆ—\"\"\"\n        if axis == 0:  # åˆ é™¤è¡Œ\n            missing_pct = self.df.isnull().sum(axis=1) / len(self.df.columns)\n            self.df = self.df[missing_pct <= threshold]\n        else:  # åˆ é™¤åˆ—\n            missing_pct = self.df.isnull().sum() / len(self.df)\n            cols_to_keep = missing_pct[missing_pct <= threshold].index\n            self.df = self.df[cols_to_keep]\n        \n        return self.df\n    \n    def fill_numeric(self, columns, method='mean'):\n        \"\"\"å¡«å……æ•°å€¼åˆ—\"\"\"\n        for col in columns:\n            if method == 'mean':\n                self.df[col].fillna(self.df[col].mean(), inplace=True)\n            elif method == 'median':\n                self.df[col].fillna(self.df[col].median(), inplace=True)\n            elif method == 'mode':\n                self.df[col].fillna(self.df[col].mode()[0], inplace=True)\n            elif method == 'forward':\n                self.df[col].fillna(method='ffill', inplace=True)\n            elif method == 'backward':\n                self.df[col].fillna(method='bfill', inplace=True)\n            elif isinstance(method, (int, float)):\n                self.df[col].fillna(method, inplace=True)\n        \n        return self.df\n    \n    def fill_categorical(self, columns, method='mode'):\n        \"\"\"å¡«å……ç±»åˆ«åˆ—\"\"\"\n        for col in columns:\n            if method == 'mode':\n                self.df[col].fillna(self.df[col].mode()[0], inplace=True)\n            elif method == 'unknown':\n                self.df[col].fillna('Unknown', inplace=True)\n            elif isinstance(method, str):\n                self.df[col].fillna(method, inplace=True)\n        \n        return self.df\n    \n    def interpolate(self, columns, method='linear'):\n        \"\"\"æ’å€¼å¡«å……\"\"\"\n        for col in columns:\n            self.df[col].interpolate(method=method, inplace=True)\n        \n        return self.df\n\n# ä½¿ç”¨ç¤ºä¾‹\nhandler = MissingValueHandler(df)\n\n# åˆ é™¤ç¼ºå¤±å€¼è¶…è¿‡50%çš„åˆ—\ndf_cleaned = handler.drop_missing(threshold=0.5, axis=1)\n\n# å¡«å……æ•°å€¼åˆ—\nnumeric_cols = ['age', 'price', 'quantity']\ndf_cleaned = handler.fill_numeric(numeric_cols, method='median')\n\n# å¡«å……ç±»åˆ«åˆ—\ncategorical_cols = ['category', 'status']\ndf_cleaned = handler.fill_categorical(categorical_cols, method='mode')\n```\n\n### 3.2 å¼‚å¸¸å€¼å¤„ç†\n\n```python\nclass OutlierHandler:\n    def __init__(self, df):\n        self.df = df.copy()\n    \n    def remove_outliers_iqr(self, columns, factor=1.5):\n        \"\"\"ä½¿ç”¨IQRæ–¹æ³•ç§»é™¤å¼‚å¸¸å€¼\"\"\"\n        for col in columns:\n            Q1 = self.df[col].quantile(0.25)\n            Q3 = self.df[col].quantile(0.75)\n            IQR = Q3 - Q1\n            \n            lower_bound = Q1 - factor * IQR\n            upper_bound = Q3 + factor * IQR\n            \n            self.df = self.df[\n                (self.df[col] >= lower_bound) & \n                (self.df[col] <= upper_bound)\n            ]\n        \n        return self.df\n    \n    def cap_outliers(self, columns, lower_percentile=0.01, upper_percentile=0.99):\n        \"\"\"æˆªæ–­å¼‚å¸¸å€¼åˆ°æŒ‡å®šç™¾åˆ†ä½\"\"\"\n        for col in columns:\n            lower = self.df[col].quantile(lower_percentile)\n            upper = self.df[col].quantile(upper_percentile)\n            \n            self.df[col] = self.df[col].clip(lower=lower, upper=upper)\n        \n        return self.df\n    \n    def transform_outliers(self, columns, method='log'):\n        \"\"\"è½¬æ¢å¼‚å¸¸å€¼\"\"\"\n        for col in columns:\n            if method == 'log':\n                self.df[col] = np.log1p(self.df[col])  # log(1+x)\n            elif method == 'sqrt':\n                self.df[col] = np.sqrt(self.df[col])\n            elif method == 'boxcox':\n                from scipy import stats\n                self.df[col], _ = stats.boxcox(self.df[col] + 1)\n        \n        return self.df\n\n# ä½¿ç”¨ç¤ºä¾‹\noutlier_handler = OutlierHandler(df_cleaned)\n\n# æ–¹æ³•1: ç§»é™¤å¼‚å¸¸å€¼\n# df_no_outliers = outlier_handler.remove_outliers_iqr(['price', 'quantity'])\n\n# æ–¹æ³•2: æˆªæ–­å¼‚å¸¸å€¼ï¼ˆæ¨èï¼‰\ndf_cleaned = outlier_handler.cap_outliers(['price', 'quantity'])\n```\n\n### 3.3 é‡å¤æ•°æ®å¤„ç†\n\n```python\n# æŸ¥çœ‹é‡å¤è®°å½•\nduplicates = df_cleaned[df_cleaned.duplicated(keep=False)]\nprint(f\"é‡å¤è®°å½•æ•°: {len(duplicates)}\")\n\n# åˆ é™¤å®Œå…¨é‡å¤çš„è®°å½•ï¼ˆä¿ç•™ç¬¬ä¸€æ¡ï¼‰\ndf_cleaned = df_cleaned.drop_duplicates(keep='first')\n\n# åŸºäºç‰¹å®šåˆ—åˆ é™¤é‡å¤\ndf_cleaned = df_cleaned.drop_duplicates(subset=['user_id', 'transaction_date'], keep='last')\n\nprint(f\"æ¸…æ´—åæ•°æ®å½¢çŠ¶: {df_cleaned.shape}\")\n```\n\n### 3.4 æ•°æ®ç±»å‹è½¬æ¢\n\n```python\nclass DataTypeConverter:\n    def __init__(self, df):\n        self.df = df.copy()\n    \n    def to_numeric(self, columns, errors='coerce'):\n        \"\"\"è½¬æ¢ä¸ºæ•°å€¼ç±»å‹\"\"\"\n        for col in columns:\n            self.df[col] = pd.to_numeric(self.df[col], errors=errors)\n        return self.df\n    \n    def to_datetime(self, columns, format=None):\n        \"\"\"è½¬æ¢ä¸ºæ—¥æœŸç±»å‹\"\"\"\n        for col in columns:\n            self.df[col] = pd.to_datetime(self.df[col], format=format, errors='coerce')\n        return self.df\n    \n    def to_category(self, columns):\n        \"\"\"è½¬æ¢ä¸ºç±»åˆ«ç±»å‹ï¼ˆèŠ‚çœå†…å­˜ï¼‰\"\"\"\n        for col in columns:\n            self.df[col] = self.df[col].astype('category')\n        return self.df\n    \n    def optimize_types(self):\n        \"\"\"è‡ªåŠ¨ä¼˜åŒ–æ•°æ®ç±»å‹\"\"\"\n        # ä¼˜åŒ–æ•´æ•°ç±»å‹\n        int_cols = self.df.select_dtypes(include=['int64']).columns\n        for col in int_cols:\n            if self.df[col].min() >= 0:\n                if self.df[col].max() <= 255:\n                    self.df[col] = self.df[col].astype('uint8')\n                elif self.df[col].max() <= 65535:\n                    self.df[col] = self.df[col].astype('uint16')\n                elif self.df[col].max() <= 4294967295:\n                    self.df[col] = self.df[col].astype('uint32')\n        \n        # ä¼˜åŒ–æµ®ç‚¹ç±»å‹\n        float_cols = self.df.select_dtypes(include=['float64']).columns\n        for col in float_cols:\n            self.df[col] = self.df[col].astype('float32')\n        \n        return self.df\n\n# ä½¿ç”¨ç¤ºä¾‹\nconverter = DataTypeConverter(df_cleaned)\ndf_cleaned = converter.to_numeric(['age', 'price'])\ndf_cleaned = converter.to_datetime(['order_date', 'delivery_date'])\ndf_cleaned = converter.to_category(['category', 'status'])\ndf_cleaned = converter.optimize_types()\n\nprint(\"\\nä¼˜åŒ–åçš„å†…å­˜ä½¿ç”¨:\")\nprint(df_cleaned.memory_usage(deep=True))\n```\n\n### 3.5 æ•°æ®æ ‡å‡†åŒ–\n\n```python\n# å­—ç¬¦ä¸²æ ‡å‡†åŒ–\ndf_cleaned['name'] = df_cleaned['name'].str.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼\ndf_cleaned['name'] = df_cleaned['name'].str.lower()  # è½¬å°å†™\ndf_cleaned['email'] = df_cleaned['email'].str.lower()  # é‚®ç®±ç»Ÿä¸€å°å†™\n\n# ç±»åˆ«å€¼æ ‡å‡†åŒ–\nstatus_mapping = {\n    'Active': 'active',\n    'ACTIVE': 'active',\n    'Inactive': 'inactive',\n    'INACTIVE': 'inactive'\n}\ndf_cleaned['status'] = df_cleaned['status'].replace(status_mapping)\n\n# æ•°å€¼æ ‡å‡†åŒ–\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler = StandardScaler()\ndf_cleaned[['age', 'price']] = scaler.fit_transform(df_cleaned[['age', 'price']])\n```\n\n## 4. æ•°æ®éªŒè¯\n\n```python\nclass DataValidator:\n    def __init__(self, df):\n        self.df = df\n        self.validation_errors = []\n    \n    def validate_not_null(self, columns):\n        \"\"\"éªŒè¯éç©º\"\"\"\n        for col in columns:\n            null_count = self.df[col].isnull().sum()\n            if null_count > 0:\n                self.validation_errors.append({\n                    'åˆ—å': col,\n                    'é”™è¯¯': f'å­˜åœ¨{null_count}ä¸ªç©ºå€¼'\n                })\n    \n    def validate_unique(self, columns):\n        \"\"\"éªŒè¯å”¯ä¸€æ€§\"\"\"\n        for col in columns:\n            dup_count = self.df[col].duplicated().sum()\n            if dup_count > 0:\n                self.validation_errors.append({\n                    'åˆ—å': col,\n                    'é”™è¯¯': f'å­˜åœ¨{dup_count}ä¸ªé‡å¤å€¼'\n                })\n    \n    def validate_range(self, column, min_val, max_val):\n        \"\"\"éªŒè¯æ•°å€¼èŒƒå›´\"\"\"\n        invalid = ((self.df[column] < min_val) | (self.df[column] > max_val)).sum()\n        if invalid > 0:\n            self.validation_errors.append({\n                'åˆ—å': column,\n                'é”™è¯¯': f'{invalid}ä¸ªå€¼ä¸åœ¨èŒƒå›´[{min_val}, {max_val}]å†…'\n            })\n    \n    def validate_format(self, column, pattern):\n        \"\"\"éªŒè¯æ ¼å¼ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰\"\"\"\n        import re\n        invalid = ~self.df[column].astype(str).str.match(pattern)\n        invalid_count = invalid.sum()\n        if invalid_count > 0:\n            self.validation_errors.append({\n                'åˆ—å': column,\n                'é”™è¯¯': f'{invalid_count}ä¸ªå€¼æ ¼å¼ä¸æ­£ç¡®'\n            })\n    \n    def get_report(self):\n        \"\"\"è·å–éªŒè¯æŠ¥å‘Š\"\"\"\n        if self.validation_errors:\n            print(\"æ•°æ®éªŒè¯å¤±è´¥:\")\n            for error in self.validation_errors:\n                print(f\"  - {error['åˆ—å']}: {error['é”™è¯¯']}\")\n            return False\n        else:\n            print(\"æ•°æ®éªŒè¯é€šè¿‡ï¼\")\n            return True\n\n# ä½¿ç”¨ç¤ºä¾‹\nvalidator = DataValidator(df_cleaned)\nvalidator.validate_not_null(['user_id', 'order_date'])\nvalidator.validate_unique(['user_id'])\nvalidator.validate_range('age', 0, 120)\nvalidator.validate_format('email', r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n\nis_valid = validator.get_report()\n```\n\n## 5. ä¿å­˜æ¸…æ´—åçš„æ•°æ®\n\n```python\n# ä¿å­˜ä¸ºCSV\ndf_cleaned.to_csv('data_cleaned.csv', index=False, encoding='utf-8-sig')\n\n# ä¿å­˜ä¸ºParquetï¼ˆæ¨èï¼Œä¿ç•™æ•°æ®ç±»å‹ï¼‰\ndf_cleaned.to_parquet('data_cleaned.parquet', index=False)\n\n# ä¿å­˜æ¸…æ´—æ—¥å¿—\ncleaning_log = {\n    'æ¸…æ´—æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'åŸå§‹æ•°æ®å½¢çŠ¶': df.shape,\n    'æ¸…æ´—åæ•°æ®å½¢çŠ¶': df_cleaned.shape,\n    'åˆ é™¤è¡Œæ•°': df.shape[0] - df_cleaned.shape[0],\n    'åˆ é™¤åˆ—æ•°': df.shape[1] - df_cleaned.shape[1],\n    'è´¨é‡æŠ¥å‘Š': report\n}\n\nimport json\nwith open('cleaning_log.json', 'w', encoding='utf-8') as f:\n    json.dump(cleaning_log, f, ensure_ascii=False, indent=2, default=str)\n\nprint(\"\\næ•°æ®æ¸…æ´—å®Œæˆï¼\")\nprint(f\"åŸå§‹æ•°æ®: {df.shape}\")\nprint(f\"æ¸…æ´—åæ•°æ®: {df_cleaned.shape}\")\nprint(f\"æ•°æ®è´¨é‡æå‡: {(1 - len(df_cleaned)/len(df)) * 100:.2f}%\")\n```\n\n## 6. æœ€ä½³å®è·µ\n\n1. **å¤‡ä»½åŸå§‹æ•°æ®**: å§‹ç»ˆä¿ç•™åŸå§‹æ•°æ®çš„å‰¯æœ¬\n2. **é€æ­¥æ¸…æ´—**: æ¯æ¬¡åªå¤„ç†ä¸€ç§é—®é¢˜ï¼Œä¾¿äºè¿½è¸ª\n3. **è®°å½•æ—¥å¿—**: è¯¦ç»†è®°å½•æ¯ä¸ªæ¸…æ´—æ­¥éª¤å’Œå‚æ•°\n4. **éªŒè¯ç»“æœ**: æ¸…æ´—åå¿…é¡»éªŒè¯æ•°æ®è´¨é‡\n5. **å¯é‡å¤æ€§**: å°†æ¸…æ´—æµç¨‹å†™æˆè„šæœ¬ï¼Œç¡®ä¿å¯é‡å¤æ‰§è¡Œ\n6. **ä¸šåŠ¡ç†è§£**: åœ¨å¤„ç†å¼‚å¸¸å€¼å‰ï¼Œå…ˆç†è§£ä¸šåŠ¡å«ä¹‰\n7. **å°å¿ƒåˆ é™¤**: åˆ é™¤æ•°æ®è¦è°¨æ…ï¼Œä¼˜å…ˆè€ƒè™‘å¡«å……æˆ–è½¬æ¢\n\n## 7. å¸¸è§é—®é¢˜\n\n**Q: ç¼ºå¤±å€¼å¤ªå¤šæ€ä¹ˆåŠï¼Ÿ**\nA: å¦‚æœç¼ºå¤±æ¯”ä¾‹>70%ï¼Œè€ƒè™‘åˆ é™¤è¯¥åˆ—ï¼›å¦‚æœæ˜¯å…³é”®åˆ—ï¼Œè€ƒè™‘è¡¥å……æ•°æ®æºæˆ–ä½¿ç”¨é«˜çº§æ’è¡¥æ–¹æ³•ï¼ˆKNNã€MICEï¼‰ã€‚\n\n**Q: å¼‚å¸¸å€¼æ˜¯é”™è¯¯è¿˜æ˜¯çœŸå®æ•°æ®ï¼Ÿ**\nA: éœ€è¦ç»“åˆä¸šåŠ¡åˆ¤æ–­ã€‚å¯ä»¥å…ˆæ ‡è®°å¼‚å¸¸å€¼ï¼Œä¸ä¸šåŠ¡ä¸“å®¶ç¡®è®¤åå†å†³å®šå¤„ç†æ–¹å¼ã€‚\n\n**Q: æ•°æ®æ¸…æ´—èŠ±è´¹æ—¶é—´å¤ªé•¿ï¼Ÿ**\nA: ä½¿ç”¨Daskæˆ–Polarså¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼›å¯¹äºè¶…å¤§æ•°æ®é›†ï¼Œè€ƒè™‘ä½¿ç”¨Sparkã€‚\n\n**Q: å¦‚ä½•å¤„ç†æ—¶é—´åºåˆ—æ•°æ®çš„ç¼ºå¤±å€¼ï¼Ÿ**\nA: ä½¿ç”¨æ’å€¼æ–¹æ³•ï¼ˆçº¿æ€§ã€æ ·æ¡ã€ARIMAé¢„æµ‹ï¼‰è€Œä¸æ˜¯ç®€å•çš„å‡å€¼å¡«å……ã€‚\n\n---\n\n**æ–‡æ¡£ç‰ˆæœ¬**: 1.0\n**æœ€åæ›´æ–°**: {{#if updateDate}}{{updateDate}}{{else}}{{currentDate}}{{/if}}\n**è´Ÿè´£äºº**: {{#if owner}}{{owner}}{{else}}æ•°æ®å›¢é˜Ÿ{{/if}}",
  "variables_schema": [
    {
      "name": "projectName",
      "type": "text",
      "label": "é¡¹ç›®åç§°",
      "default": "æ•°æ®æ¸…æ´—é¡¹ç›®",
      "required": true,
      "description": "æ•°æ®æ¸…æ´—é¡¹ç›®çš„åç§°"
    },
    {
      "name": "dataSource",
      "type": "text",
      "label": "æ•°æ®æº",
      "default": "data/raw_data.csv",
      "required": true,
      "description": "åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„"
    },
    {
      "name": "dataScale",
      "type": "select",
      "label": "æ•°æ®è§„æ¨¡",
      "default": "ä¸­ç­‰",
      "required": true,
      "options": [
        {"value": "å°å‹ï¼ˆ<10ä¸‡è¡Œï¼‰", "label": "å°å‹ï¼ˆ<10ä¸‡è¡Œï¼‰"},
        {"value": "ä¸­ç­‰ï¼ˆ10-100ä¸‡è¡Œï¼‰", "label": "ä¸­ç­‰ï¼ˆ10-100ä¸‡è¡Œï¼‰"},
        {"value": "å¤§å‹ï¼ˆ>100ä¸‡è¡Œï¼‰", "label": "å¤§å‹ï¼ˆ>100ä¸‡è¡Œï¼‰"}
      ],
      "description": "æ•°æ®é›†çš„å¤§å°è§„æ¨¡"
    },
    {
      "name": "cleaningGoal",
      "type": "textarea",
      "label": "æ¸…æ´—ç›®æ ‡",
      "default": "1. å¤„ç†ç¼ºå¤±å€¼\n2. ç§»é™¤å¼‚å¸¸å€¼\n3. åˆ é™¤é‡å¤è®°å½•\n4. ç»Ÿä¸€æ•°æ®æ ¼å¼",
      "required": true,
      "description": "æ•°æ®æ¸…æ´—çš„å…·ä½“ç›®æ ‡"
    },
    {
      "name": "qualityStandard",
      "type": "text",
      "label": "è´¨é‡æ ‡å‡†",
      "default": "ç¼ºå¤±å€¼<5%, å¼‚å¸¸å€¼<1%, æ— é‡å¤è®°å½•",
      "required": false,
      "description": "æ•°æ®è´¨é‡éªŒæ”¶æ ‡å‡†"
    },
    {
      "name": "updateDate",
      "type": "text",
      "label": "æ›´æ–°æ—¥æœŸ",
      "default": "",
      "required": false,
      "description": "æ–‡æ¡£æœ€åæ›´æ–°æ—¥æœŸï¼ˆå¯é€‰ï¼‰"
    },
    {
      "name": "owner",
      "type": "text",
      "label": "è´Ÿè´£äºº",
      "default": "",
      "required": false,
      "description": "é¡¹ç›®è´Ÿè´£äººå§“åï¼ˆå¯é€‰ï¼‰"
    }
  ],
  "file_structure": {
    "type": "folder",
    "name": "{{projectName}}",
    "children": [
      {
        "type": "folder",
        "name": "data",
        "children": [
          {"type": "file", "name": "raw_data.csv", "description": "åŸå§‹æ•°æ®"},
          {"type": "file", "name": "cleaned_data.csv", "description": "æ¸…æ´—åæ•°æ®"},
          {"type": "file", "name": "cleaning_log.json", "description": "æ¸…æ´—æ—¥å¿—"}
        ]
      },
      {
        "type": "folder",
        "name": "scripts",
        "children": [
          {"type": "file", "name": "data_quality_check.py", "description": "æ•°æ®è´¨é‡æ£€æŸ¥è„šæœ¬"},
          {"type": "file", "name": "data_cleaning.py", "description": "æ•°æ®æ¸…æ´—ä¸»è„šæœ¬"}
        ]
      },
      {
        "type": "folder",
        "name": "reports",
        "children": [
          {"type": "file", "name": "quality_report.html", "description": "è´¨é‡æŠ¥å‘Š"}
        ]
      }
    ]
  },
  "is_builtin": true,
  "author": "ChainlessChain Team",
  "version": "1.0.0",
  "usage_count": 0,
  "rating": 0,
  "rating_count": 0,
  "created_at": "2024-01-15T10:00:00Z",
  "updated_at": "2024-01-15T10:00:00Z"
}
