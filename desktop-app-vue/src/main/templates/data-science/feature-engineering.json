{
  "id": "tpl_data_science_features_006",
  "name": "feature_engineering",
  "display_name": "ç‰¹å¾å·¥ç¨‹æ–‡æ¡£",
  "description": "å…¨é¢çš„ç‰¹å¾å·¥ç¨‹æ¨¡æ¿ï¼ŒåŒ…å«ç‰¹å¾æå–ã€ç‰¹å¾é€‰æ‹©ã€ç‰¹å¾æ„é€ ã€ç‰¹å¾è½¬æ¢ç­‰æŠ€æœ¯ï¼Œé€‚ç”¨äºæœºå™¨å­¦ä¹ å’Œæ•°æ®æŒ–æ˜é¡¹ç›®",
  "icon": "ğŸ”§",
  "category": "data-science",
  "subcategory": "ç‰¹å¾å·¥ç¨‹",
  "tags": ["ç‰¹å¾å·¥ç¨‹", "ç‰¹å¾é€‰æ‹©", "ç‰¹å¾æå–", "é™ç»´", "æœºå™¨å­¦ä¹ "],
  "project_type": "data",
  "prompt_template": "è¯·å¸®æˆ‘ç”Ÿæˆç‰¹å¾å·¥ç¨‹æ–‡æ¡£ï¼Œè¦æ±‚:\n\n# {{projectName}} - ç‰¹å¾å·¥ç¨‹æ–‡æ¡£\n\n## 1. é¡¹ç›®æ¦‚è¿°\n\n**ä»»åŠ¡ç±»å‹**: {{taskType}}\n**ç›®æ ‡å˜é‡**: {{targetVariable}}\n**ç‰¹å¾æ•°é‡**: {{featureCount}}\n**å·¥ç¨‹ç›®æ ‡**: {{engineeringGoal}}\n\n## 2. ç‰¹å¾ç±»å‹åˆ†æ\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\nfrom sklearn.decomposition import PCA\n\n# åŠ è½½æ•°æ®\ndf = pd.read_csv('{{dataPath}}')\n\n# åˆ†æç‰¹å¾ç±»å‹\nprint(\"æ•°å€¼ç‰¹å¾:\", df.select_dtypes(include=[np.number]).columns.tolist())\nprint(\"ç±»åˆ«ç‰¹å¾:\", df.select_dtypes(include=['object', 'category']).columns.tolist())\nprint(\"æ—¥æœŸç‰¹å¾:\", df.select_dtypes(include=['datetime']).columns.tolist())\n```\n\n## 3. ç‰¹å¾æå–\n\n### 3.1 æ•°å€¼ç‰¹å¾æå–\n\n```python\nclass NumericFeatureExtractor:\n    \"\"\"æ•°å€¼ç‰¹å¾æå–å™¨\"\"\"\n    \n    @staticmethod\n    def basic_stats(df, columns):\n        \"\"\"åŸºç¡€ç»Ÿè®¡ç‰¹å¾\"\"\"\n        features = pd.DataFrame()\n        \n        for col in columns:\n            features[f'{col}_mean'] = df[col].mean()\n            features[f'{col}_std'] = df[col].std()\n            features[f'{col}_min'] = df[col].min()\n            features[f'{col}_max'] = df[col].max()\n            features[f'{col}_median'] = df[col].median()\n            features[f'{col}_q25'] = df[col].quantile(0.25)\n            features[f'{col}_q75'] = df[col].quantile(0.75)\n            features[f'{col}_iqr'] = features[f'{col}_q75'] - features[f'{col}_q25']\n        \n        return features\n    \n    @staticmethod\n    def binning_features(df, column, bins=5, labels=None):\n        \"\"\"åˆ†ç®±ç‰¹å¾\"\"\"\n        if labels is None:\n            labels = [f'{column}_bin_{i}' for i in range(bins)]\n        \n        df[f'{column}_binned'] = pd.cut(df[column], bins=bins, labels=labels)\n        return df\n    \n    @staticmethod\n    def polynomial_features(df, columns, degree=2):\n        \"\"\"å¤šé¡¹å¼ç‰¹å¾\"\"\"\n        from sklearn.preprocessing import PolynomialFeatures\n        \n        poly = PolynomialFeatures(degree=degree, include_bias=False)\n        poly_features = poly.fit_transform(df[columns])\n        \n        feature_names = poly.get_feature_names_out(columns)\n        poly_df = pd.DataFrame(poly_features, columns=feature_names, index=df.index)\n        \n        return pd.concat([df, poly_df], axis=1)\n    \n    @staticmethod\n    def ratio_features(df, numerator_cols, denominator_cols):\n        \"\"\"æ¯”ç‡ç‰¹å¾\"\"\"\n        for num_col in numerator_cols:\n            for den_col in denominator_cols:\n                if num_col != den_col:\n                    df[f'{num_col}_div_{den_col}'] = df[num_col] / (df[den_col] + 1e-10)\n        \n        return df\n    \n    @staticmethod\n    def difference_features(df, columns):\n        \"\"\"å·®å€¼ç‰¹å¾\"\"\"\n        for i, col1 in enumerate(columns):\n            for col2 in columns[i+1:]:\n                df[f'{col1}_minus_{col2}'] = df[col1] - df[col2]\n        \n        return df\n    \n    @staticmethod\n    def aggregate_features(df, group_col, agg_cols, agg_funcs=['mean', 'sum', 'max', 'min', 'std']):\n        \"\"\"åˆ†ç»„èšåˆç‰¹å¾\"\"\"\n        agg_dict = {col: agg_funcs for col in agg_cols}\n        \n        grouped = df.groupby(group_col).agg(agg_dict)\n        grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n        grouped = grouped.reset_index()\n        \n        df = df.merge(grouped, on=group_col, how='left')\n        return df\n\n# ä½¿ç”¨ç¤ºä¾‹\nextractor = NumericFeatureExtractor()\n\n# åˆ†ç®±\ndf = extractor.binning_features(df, 'age', bins=5)\n\n# å¤šé¡¹å¼ç‰¹å¾\ndf = extractor.polynomial_features(df, ['feature1', 'feature2'], degree=2)\n\n# æ¯”ç‡ç‰¹å¾\ndf = extractor.ratio_features(df, ['sales'], ['cost', 'quantity'])\n\n# èšåˆç‰¹å¾\ndf = extractor.aggregate_features(df, 'user_id', ['amount', 'quantity'])\n```\n\n### 3.2 ç±»åˆ«ç‰¹å¾æå–\n\n```python\nclass CategoricalFeatureExtractor:\n    \"\"\"ç±»åˆ«ç‰¹å¾æå–å™¨\"\"\"\n    \n    @staticmethod\n    def label_encoding(df, columns):\n        \"\"\"æ ‡ç­¾ç¼–ç \"\"\"\n        le_dict = {}\n        \n        for col in columns:\n            le = LabelEncoder()\n            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n            le_dict[col] = le\n        \n        return df, le_dict\n    \n    @staticmethod\n    def onehot_encoding(df, columns, drop_first=True):\n        \"\"\"ç‹¬çƒ­ç¼–ç \"\"\"\n        df = pd.get_dummies(df, columns=columns, drop_first=drop_first, prefix=columns)\n        return df\n    \n    @staticmethod\n    def frequency_encoding(df, columns):\n        \"\"\"é¢‘ç‡ç¼–ç \"\"\"\n        for col in columns:\n            freq = df[col].value_counts(normalize=True)\n            df[f'{col}_freq'] = df[col].map(freq)\n        \n        return df\n    \n    @staticmethod\n    def target_encoding(df, columns, target, smooth=0):\n        \"\"\"ç›®æ ‡ç¼–ç ï¼ˆé€‚ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼‰\"\"\"\n        for col in columns:\n            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡å‡å€¼\n            target_mean = df.groupby(col)[target].mean()\n            \n            if smooth > 0:\n                # å¹³æ»‘å¤„ç†\n                global_mean = df[target].mean()\n                counts = df[col].value_counts()\n                \n                smoothed_mean = (target_mean * counts + global_mean * smooth) / (counts + smooth)\n                df[f'{col}_target_encoded'] = df[col].map(smoothed_mean)\n            else:\n                df[f'{col}_target_encoded'] = df[col].map(target_mean)\n        \n        return df\n    \n    @staticmethod\n    def count_encoding(df, columns):\n        \"\"\"è®¡æ•°ç¼–ç \"\"\"\n        for col in columns:\n            counts = df[col].value_counts()\n            df[f'{col}_count'] = df[col].map(counts)\n        \n        return df\n    \n    @staticmethod\n    def combination_features(df, cat_columns):\n        \"\"\"ç»„åˆç‰¹å¾\"\"\"\n        for i, col1 in enumerate(cat_columns):\n            for col2 in cat_columns[i+1:]:\n                df[f'{col1}_{col2}_combo'] = df[col1].astype(str) + '_' + df[col2].astype(str)\n        \n        return df\n\n# ä½¿ç”¨ç¤ºä¾‹\ncat_extractor = CategoricalFeatureExtractor()\n\n# é¢‘ç‡ç¼–ç \ndf = cat_extractor.frequency_encoding(df, ['category', 'brand'])\n\n# ç›®æ ‡ç¼–ç ï¼ˆå‡è®¾ç›®æ ‡å˜é‡æ˜¯'target'ï¼‰\ndf = cat_extractor.target_encoding(df, ['category'], 'target', smooth=10)\n\n# ç»„åˆç‰¹å¾\ndf = cat_extractor.combination_features(df, ['category', 'subcategory'])\n\n# ç‹¬çƒ­ç¼–ç ï¼ˆå¯¹ä½åŸºæ•°ç‰¹å¾ï¼‰\nlow_cardinality_cols = [col for col in df.select_dtypes(include=['object']).columns \n                        if df[col].nunique() < 10]\ndf = cat_extractor.onehot_encoding(df, low_cardinality_cols)\n```\n\n### 3.3 æ—¶é—´ç‰¹å¾æå–\n\n```python\nclass TimeFeatureExtractor:\n    \"\"\"æ—¶é—´ç‰¹å¾æå–å™¨\"\"\"\n    \n    @staticmethod\n    def extract_datetime_features(df, datetime_col):\n        \"\"\"æå–æ—¥æœŸæ—¶é—´ç‰¹å¾\"\"\"\n        df[datetime_col] = pd.to_datetime(df[datetime_col])\n        \n        # åŸºç¡€æ—¶é—´ç‰¹å¾\n        df[f'{datetime_col}_year'] = df[datetime_col].dt.year\n        df[f'{datetime_col}_month'] = df[datetime_col].dt.month\n        df[f'{datetime_col}_day'] = df[datetime_col].dt.day\n        df[f'{datetime_col}_dayofweek'] = df[datetime_col].dt.dayofweek\n        df[f'{datetime_col}_hour'] = df[datetime_col].dt.hour\n        df[f'{datetime_col}_minute'] = df[datetime_col].dt.minute\n        \n        # æ˜¯å¦å‘¨æœ«\n        df[f'{datetime_col}_is_weekend'] = (df[f'{datetime_col}_dayofweek'] >= 5).astype(int)\n        \n        # å­£åº¦\n        df[f'{datetime_col}_quarter'] = df[datetime_col].dt.quarter\n        \n        # æ˜¯å¦æœˆåˆ/æœˆæœ«\n        df[f'{datetime_col}_is_month_start'] = df[datetime_col].dt.is_month_start.astype(int)\n        df[f'{datetime_col}_is_month_end'] = df[datetime_col].dt.is_month_end.astype(int)\n        \n        # ä¸€å¹´ä¸­çš„ç¬¬å‡ å¤©\n        df[f'{datetime_col}_dayofyear'] = df[datetime_col].dt.dayofyear\n        \n        # ä¸€å¹´ä¸­çš„ç¬¬å‡ å‘¨\n        df[f'{datetime_col}_weekofyear'] = df[datetime_col].dt.isocalendar().week\n        \n        return df\n    \n    @staticmethod\n    def time_since_features(df, datetime_col, reference_date=None):\n        \"\"\"è·ç¦»å‚è€ƒæ—¥æœŸçš„æ—¶é—´å·®\"\"\"\n        if reference_date is None:\n            reference_date = pd.Timestamp.now()\n        \n        df[datetime_col] = pd.to_datetime(df[datetime_col])\n        time_diff = reference_date - df[datetime_col]\n        \n        df[f'{datetime_col}_days_since'] = time_diff.dt.days\n        df[f'{datetime_col}_weeks_since'] = time_diff.dt.days // 7\n        df[f'{datetime_col}_months_since'] = time_diff.dt.days // 30\n        \n        return df\n    \n    @staticmethod\n    def cyclic_encoding(df, column, max_val):\n        \"\"\"å¾ªç¯ç‰¹å¾ç¼–ç ï¼ˆé€‚ç”¨äºå°æ—¶ã€æœˆä»½ç­‰ï¼‰\"\"\"\n        df[f'{column}_sin'] = np.sin(2 * np.pi * df[column] / max_val)\n        df[f'{column}_cos'] = np.cos(2 * np.pi * df[column] / max_val)\n        return df\n\n# ä½¿ç”¨ç¤ºä¾‹\ntime_extractor = TimeFeatureExtractor()\n\n# æå–æ—¥æœŸæ—¶é—´ç‰¹å¾\ndf = time_extractor.extract_datetime_features(df, 'order_date')\n\n# è·ä»Šå¤©æ•°\ndf = time_extractor.time_since_features(df, 'order_date')\n\n# å¾ªç¯ç¼–ç ï¼ˆæœˆä»½ï¼‰\ndf = time_extractor.cyclic_encoding(df, 'order_date_month', 12)\n```\n\n### 3.4 æ–‡æœ¬ç‰¹å¾æå–\n\n```python\nclass TextFeatureExtractor:\n    \"\"\"æ–‡æœ¬ç‰¹å¾æå–å™¨\"\"\"\n    \n    @staticmethod\n    def basic_text_features(df, text_col):\n        \"\"\"åŸºç¡€æ–‡æœ¬ç‰¹å¾\"\"\"\n        df[f'{text_col}_length'] = df[text_col].astype(str).str.len()\n        df[f'{text_col}_word_count'] = df[text_col].astype(str).str.split().str.len()\n        df[f'{text_col}_unique_words'] = df[text_col].astype(str).apply(\n            lambda x: len(set(x.split()))\n        )\n        df[f'{text_col}_avg_word_length'] = df[text_col].astype(str).apply(\n            lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0\n        )\n        \n        return df\n    \n    @staticmethod\n    def tfidf_features(df, text_col, max_features=100):\n        \"\"\"TF-IDFç‰¹å¾\"\"\"\n        from sklearn.feature_extraction.text import TfidfVectorizer\n        \n        vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n        tfidf_matrix = vectorizer.fit_transform(df[text_col].astype(str))\n        \n        tfidf_df = pd.DataFrame(\n            tfidf_matrix.toarray(),\n            columns=[f'{text_col}_tfidf_{i}' for i in range(tfidf_matrix.shape[1])],\n            index=df.index\n        )\n        \n        return pd.concat([df, tfidf_df], axis=1)\n    \n    @staticmethod\n    def sentiment_features(df, text_col):\n        \"\"\"æƒ…æ„Ÿç‰¹å¾ï¼ˆéœ€è¦å®‰è£…textblobï¼‰\"\"\"\n        try:\n            from textblob import TextBlob\n            \n            df[f'{text_col}_sentiment'] = df[text_col].astype(str).apply(\n                lambda x: TextBlob(x).sentiment.polarity\n            )\n            df[f'{text_col}_subjectivity'] = df[text_col].astype(str).apply(\n                lambda x: TextBlob(x).sentiment.subjectivity\n            )\n        except ImportError:\n            print(\"éœ€è¦å®‰è£…textblob: pip install textblob\")\n        \n        return df\n\n# ä½¿ç”¨ç¤ºä¾‹\ntext_extractor = TextFeatureExtractor()\ndf = text_extractor.basic_text_features(df, 'description')\ndf = text_extractor.tfidf_features(df, 'description', max_features=50)\n```\n\n## 4. ç‰¹å¾è½¬æ¢\n\n```python\nclass FeatureTransformer:\n    \"\"\"ç‰¹å¾è½¬æ¢å™¨\"\"\"\n    \n    @staticmethod\n    def log_transform(df, columns):\n        \"\"\"å¯¹æ•°è½¬æ¢ï¼ˆå¤„ç†å³åæ•°æ®ï¼‰\"\"\"\n        for col in columns:\n            df[f'{col}_log'] = np.log1p(df[col])  # log(1+x) é¿å…log(0)\n        return df\n    \n    @staticmethod\n    def sqrt_transform(df, columns):\n        \"\"\"å¹³æ–¹æ ¹è½¬æ¢\"\"\"\n        for col in columns:\n            df[f'{col}_sqrt'] = np.sqrt(df[col])\n        return df\n    \n    @staticmethod\n    def boxcox_transform(df, columns):\n        \"\"\"Box-Coxè½¬æ¢\"\"\"\n        from scipy import stats\n        \n        for col in columns:\n            # ç¡®ä¿æ•°æ®ä¸ºæ­£\n            if (df[col] > 0).all():\n                df[f'{col}_boxcox'], _ = stats.boxcox(df[col])\n            else:\n                print(f\"è­¦å‘Š: {col} åŒ…å«éæ­£å€¼ï¼Œè·³è¿‡Box-Coxè½¬æ¢\")\n        \n        return df\n    \n    @staticmethod\n    def standardization(df, columns):\n        \"\"\"æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰\"\"\"\n        scaler = StandardScaler()\n        df[columns] = scaler.fit_transform(df[columns])\n        return df, scaler\n    \n    @staticmethod\n    def normalization(df, columns, feature_range=(0, 1)):\n        \"\"\"å½’ä¸€åŒ–ï¼ˆMin-Maxï¼‰\"\"\"\n        scaler = MinMaxScaler(feature_range=feature_range)\n        df[columns] = scaler.fit_transform(df[columns])\n        return df, scaler\n    \n    @staticmethod\n    def robust_scaling(df, columns):\n        \"\"\"é²æ£’ç¼©æ”¾ï¼ˆå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼‰\"\"\"\n        from sklearn.preprocessing import RobustScaler\n        \n        scaler = RobustScaler()\n        df[columns] = scaler.fit_transform(df[columns])\n        return df, scaler\n\n# ä½¿ç”¨ç¤ºä¾‹\ntransformer = FeatureTransformer()\n\n# å¯¹å³åç‰¹å¾è¿›è¡Œå¯¹æ•°è½¬æ¢\nskewed_features = ['price', 'sales_volume']\ndf = transformer.log_transform(df, skewed_features)\n\n# æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾\nnumeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\ndf, scaler = transformer.standardization(df, numeric_features)\n```\n\n## 5. ç‰¹å¾é€‰æ‹©\n\n```python\nclass FeatureSelector:\n    \"\"\"ç‰¹å¾é€‰æ‹©å™¨\"\"\"\n    \n    @staticmethod\n    def remove_low_variance(df, threshold=0.01):\n        \"\"\"ç§»é™¤ä½æ–¹å·®ç‰¹å¾\"\"\"\n        from sklearn.feature_selection import VarianceThreshold\n        \n        selector = VarianceThreshold(threshold=threshold)\n        selector.fit(df)\n        \n        selected_features = df.columns[selector.get_support()].tolist()\n        return df[selected_features], selected_features\n    \n    @staticmethod\n    def remove_correlated(df, threshold=0.95):\n        \"\"\"ç§»é™¤é«˜ç›¸å…³ç‰¹å¾\"\"\"\n        corr_matrix = df.corr().abs()\n        \n        # æ‰¾å‡ºé«˜ç›¸å…³çš„ç‰¹å¾å¯¹\n        upper_tri = corr_matrix.where(\n            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n        )\n        \n        to_drop = [column for column in upper_tri.columns \n                   if any(upper_tri[column] > threshold)]\n        \n        return df.drop(columns=to_drop), to_drop\n    \n    @staticmethod\n    def univariate_selection(X, y, k=10, task='classification'):\n        \"\"\"å•å˜é‡ç‰¹å¾é€‰æ‹©\"\"\"\n        if task == 'classification':\n            selector = SelectKBest(f_classif, k=k)\n        else:\n            from sklearn.feature_selection import f_regression\n            selector = SelectKBest(f_regression, k=k)\n        \n        X_selected = selector.fit_transform(X, y)\n        selected_features = X.columns[selector.get_support()].tolist()\n        \n        # ç‰¹å¾å¾—åˆ†\n        scores = pd.DataFrame({\n            'feature': X.columns,\n            'score': selector.scores_\n        }).sort_values('score', ascending=False)\n        \n        return X_selected, selected_features, scores\n    \n    @staticmethod\n    def mutual_info_selection(X, y, k=10, task='classification'):\n        \"\"\"äº’ä¿¡æ¯ç‰¹å¾é€‰æ‹©\"\"\"\n        from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n        \n        if task == 'classification':\n            mi_scores = mutual_info_classif(X, y)\n        else:\n            mi_scores = mutual_info_regression(X, y)\n        \n        mi_df = pd.DataFrame({\n            'feature': X.columns,\n            'mi_score': mi_scores\n        }).sort_values('mi_score', ascending=False)\n        \n        top_features = mi_df.head(k)['feature'].tolist()\n        \n        return X[top_features], top_features, mi_df\n    \n    @staticmethod\n    def recursive_feature_elimination(X, y, n_features=10, estimator=None):\n        \"\"\"é€’å½’ç‰¹å¾æ¶ˆé™¤\"\"\"\n        from sklearn.feature_selection import RFE\n        from sklearn.ensemble import RandomForestClassifier\n        \n        if estimator is None:\n            estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n        \n        selector = RFE(estimator, n_features_to_select=n_features, step=1)\n        selector.fit(X, y)\n        \n        selected_features = X.columns[selector.get_support()].tolist()\n        \n        ranking = pd.DataFrame({\n            'feature': X.columns,\n            'ranking': selector.ranking_\n        }).sort_values('ranking')\n        \n        return X[selected_features], selected_features, ranking\n    \n    @staticmethod\n    def tree_based_selection(X, y, threshold='median', estimator=None):\n        \"\"\"åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©\"\"\"\n        from sklearn.ensemble import RandomForestClassifier\n        from sklearn.feature_selection import SelectFromModel\n        \n        if estimator is None:\n            estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n        \n        estimator.fit(X, y)\n        \n        # ç‰¹å¾é‡è¦æ€§\n        importance_df = pd.DataFrame({\n            'feature': X.columns,\n            'importance': estimator.feature_importances_\n        }).sort_values('importance', ascending=False)\n        \n        # é€‰æ‹©ç‰¹å¾\n        selector = SelectFromModel(estimator, threshold=threshold, prefit=True)\n        X_selected = selector.transform(X)\n        selected_features = X.columns[selector.get_support()].tolist()\n        \n        return X_selected, selected_features, importance_df\n\n# ä½¿ç”¨ç¤ºä¾‹\nselector = FeatureSelector()\n\n# ç§»é™¤ä½æ–¹å·®ç‰¹å¾\ndf_selected, low_var_features = selector.remove_low_variance(df)\n\n# ç§»é™¤é«˜ç›¸å…³ç‰¹å¾\ndf_selected, corr_features = selector.remove_correlated(df_selected, threshold=0.9)\n\n# åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©\nX = df_selected.drop('target', axis=1)\ny = df_selected['target']\nX_selected, selected_features, importance = selector.tree_based_selection(X, y)\n\nprint(f\"\\nç‰¹å¾é€‰æ‹©å: {len(selected_features)} ä¸ªç‰¹å¾\")\nprint(\"\\nç‰¹å¾é‡è¦æ€§Top 10:\")\nprint(importance.head(10))\n```\n\n## 6. é™ç»´\n\n```python\n# PCAé™ç»´\npca = PCA(n_components=0.95)  # ä¿ç•™95%çš„æ–¹å·®\nX_pca = pca.fit_transform(X_selected)\n\nprint(f\"PCAåç»´åº¦: {X_pca.shape[1]}\")\nprint(f\"è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_.sum():.2%}\")\n\n# å¯è§†åŒ–å‰ä¸¤ä¸ªä¸»æˆåˆ†\nplt.figure(figsize=(10, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.6)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA - å‰ä¸¤ä¸ªä¸»æˆåˆ†')\nplt.colorbar(label='Target')\nplt.show()\n```\n\n## 7. ç‰¹å¾å·¥ç¨‹æµæ°´çº¿\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# å®šä¹‰æ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾\nnumeric_features = ['age', 'income', 'credit_score']\ncategorical_features = ['gender', 'occupation', 'city']\n\n# æ•°å€¼ç‰¹å¾æµæ°´çº¿\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\n# ç±»åˆ«ç‰¹å¾æµæ°´çº¿\nfrom sklearn.preprocessing import OneHotEncoder\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# ç»„åˆæµæ°´çº¿\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# å®Œæ•´æµæ°´çº¿ï¼ˆåŒ…å«æ¨¡å‹ï¼‰\nfrom sklearn.ensemble import RandomForestClassifier\n\nfull_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n# è®­ç»ƒ\nfull_pipeline.fit(X_train, y_train)\n\n# é¢„æµ‹\ny_pred = full_pipeline.predict(X_test)\n```\n\n## 8. ç‰¹å¾é‡è¦æ€§åˆ†æ\n\n```python\n# ä½¿ç”¨SHAPè¿›è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ\nimport shap\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# SHAPå€¼\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\n# å¯è§†åŒ–\nshap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")\nshap.summary_plot(shap_values[1], X_test)\n```\n\n## 9. æœ€ä½³å®è·µ\n\n1. **ç†è§£ä¸šåŠ¡**: ç‰¹å¾å·¥ç¨‹åº”åŸºäºä¸šåŠ¡ç†è§£ï¼Œè€Œä¸æ˜¯ç›²ç›®ç”Ÿæˆç‰¹å¾\n2. **é¿å…æ•°æ®æ³„éœ²**: ä¸è¦ä½¿ç”¨æœªæ¥ä¿¡æ¯æˆ–ç›®æ ‡å˜é‡è¡ç”Ÿç‰¹å¾\n3. **äº¤å‰éªŒè¯**: åœ¨äº¤å‰éªŒè¯ä¸­è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œé¿å…è¿‡æ‹Ÿåˆ\n4. **ç‰¹å¾å­˜å‚¨**: ä¿å­˜ç‰¹å¾å·¥ç¨‹çš„è½¬æ¢å™¨ï¼ˆscalerã€encoderç­‰ï¼‰\n5. **ç‰¹å¾ç‰ˆæœ¬åŒ–**: è®°å½•ç‰¹å¾å·¥ç¨‹çš„ç‰ˆæœ¬å’Œå˜æ›´\n6. **å¢é‡ç‰¹å¾å·¥ç¨‹**: é€æ­¥æ·»åŠ ç‰¹å¾ï¼Œè¯„ä¼°æ¯ä¸ªç‰¹å¾çš„è´¡çŒ®\n7. **ç‰¹å¾ç›‘æ§**: åœ¨ç”Ÿäº§ç¯å¢ƒç›‘æ§ç‰¹å¾åˆ†å¸ƒçš„å˜åŒ–\n\n## 10. å¸¸è§é—®é¢˜\n\n**Q: åº”è¯¥ç”Ÿæˆå¤šå°‘ç‰¹å¾ï¼Ÿ**\nA: éµå¾ª\"è¶Šå°‘è¶Šå¥½\"åŸåˆ™ã€‚è¿‡å¤šç‰¹å¾ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆå’Œè®¡ç®—æˆæœ¬å¢åŠ ã€‚ä½¿ç”¨ç‰¹å¾é€‰æ‹©æ–¹æ³•ç­›é€‰æœ€é‡è¦çš„ç‰¹å¾ã€‚\n\n**Q: å¦‚ä½•å¤„ç†é«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼Ÿ**\nA: ä½¿ç”¨ç›®æ ‡ç¼–ç ã€é¢‘ç‡ç¼–ç æˆ–å“ˆå¸Œç¼–ç ï¼Œé¿å…ä½¿ç”¨ç‹¬çƒ­ç¼–ç ï¼ˆä¼šäº§ç”Ÿå¤§é‡ç¨€ç–ç‰¹å¾ï¼‰ã€‚\n\n**Q: æ—¶é—´åºåˆ—æ•°æ®å¦‚ä½•è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Ÿ**\nA: æå–æ»åç‰¹å¾ã€æ»šåŠ¨çª—å£ç»Ÿè®¡ã€æ—¶é—´è¶‹åŠ¿ç‰¹å¾ã€‚æ³¨æ„é¿å…æœªæ¥æ•°æ®æ³„éœ²ã€‚\n\n**Q: å¦‚ä½•è¯„ä¼°ç‰¹å¾è´¨é‡ï¼Ÿ**\nA: ä½¿ç”¨ç‰¹å¾é‡è¦æ€§ã€äº’ä¿¡æ¯ã€SHAPå€¼ç­‰æ–¹æ³•ã€‚æœ€ç»ˆä»¥æ¨¡å‹æ€§èƒ½ï¼ˆäº¤å‰éªŒè¯ï¼‰ä¸ºå‡†ã€‚\n\n---\n\n**æ–‡æ¡£ç‰ˆæœ¬**: 1.0\n**æœ€åæ›´æ–°**: {{#if updateDate}}{{updateDate}}{{else}}{{currentDate}}{{/if}}\n**è´Ÿè´£äºº**: {{#if owner}}{{owner}}{{else}}æ•°æ®ç§‘å­¦å›¢é˜Ÿ{{/if}}",
  "variables_schema": [
    {
      "name": "projectName",
      "type": "text",
      "label": "é¡¹ç›®åç§°",
      "default": "ç‰¹å¾å·¥ç¨‹é¡¹ç›®",
      "required": true,
      "description": "ç‰¹å¾å·¥ç¨‹é¡¹ç›®çš„åç§°"
    },
    {
      "name": "taskType",
      "type": "select",
      "label": "ä»»åŠ¡ç±»å‹",
      "default": "åˆ†ç±»",
      "required": true,
      "options": [
        {"value": "åˆ†ç±»", "label": "åˆ†ç±»"},
        {"value": "å›å½’", "label": "å›å½’"},
        {"value": "èšç±»", "label": "èšç±»"},
        {"value": "æ—¶é—´åºåˆ—é¢„æµ‹", "label": "æ—¶é—´åºåˆ—é¢„æµ‹"}
      ],
      "description": "æœºå™¨å­¦ä¹ ä»»åŠ¡ç±»å‹"
    },
    {
      "name": "targetVariable",
      "type": "text",
      "label": "ç›®æ ‡å˜é‡",
      "default": "target",
      "required": true,
      "description": "ç›®æ ‡å˜é‡åç§°"
    },
    {
      "name": "featureCount",
      "type": "text",
      "label": "åˆå§‹ç‰¹å¾æ•°é‡",
      "default": "50",
      "required": false,
      "description": "åŸå§‹æ•°æ®çš„ç‰¹å¾æ•°é‡"
    },
    {
      "name": "engineeringGoal",
      "type": "textarea",
      "label": "å·¥ç¨‹ç›®æ ‡",
      "default": "1. æå–æœ‰æ„ä¹‰çš„ç‰¹å¾\n2. é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾\n3. æå‡æ¨¡å‹æ€§èƒ½",
      "required": true,
      "description": "ç‰¹å¾å·¥ç¨‹çš„å…·ä½“ç›®æ ‡"
    },
    {
      "name": "dataPath",
      "type": "text",
      "label": "æ•°æ®è·¯å¾„",
      "default": "data/processed_data.csv",
      "required": false,
      "description": "å¤„ç†åæ•°æ®æ–‡ä»¶è·¯å¾„"
    },
    {
      "name": "updateDate",
      "type": "text",
      "label": "æ›´æ–°æ—¥æœŸ",
      "default": "",
      "required": false,
      "description": "æ–‡æ¡£æœ€åæ›´æ–°æ—¥æœŸï¼ˆå¯é€‰ï¼‰"
    },
    {
      "name": "owner",
      "type": "text",
      "label": "è´Ÿè´£äºº",
      "default": "",
      "required": false,
      "description": "é¡¹ç›®è´Ÿè´£äººå§“åï¼ˆå¯é€‰ï¼‰"
    }
  ],
  "file_structure": {
    "type": "folder",
    "name": "{{projectName}}",
    "children": [
      {
        "type": "folder",
        "name": "data",
        "children": [
          {"type": "file", "name": "raw_data.csv", "description": "åŸå§‹æ•°æ®"},
          {"type": "file", "name": "processed_data.csv", "description": "é¢„å¤„ç†åæ•°æ®"},
          {"type": "file", "name": "engineered_features.csv", "description": "å·¥ç¨‹ç‰¹å¾æ•°æ®"}
        ]
      },
      {
        "type": "folder",
        "name": "scripts",
        "children": [
          {"type": "file", "name": "feature_extraction.py", "description": "ç‰¹å¾æå–è„šæœ¬"},
          {"type": "file", "name": "feature_selection.py", "description": "ç‰¹å¾é€‰æ‹©è„šæœ¬"},
          {"type": "file", "name": "feature_pipeline.py", "description": "ç‰¹å¾æµæ°´çº¿"}
        ]
      },
      {
        "type": "folder",
        "name": "models",
        "children": [
          {"type": "file", "name": "scaler.pkl", "description": "ç‰¹å¾ç¼©æ”¾å™¨"},
          {"type": "file", "name": "encoder.pkl", "description": "ç‰¹å¾ç¼–ç å™¨"}
        ]
      },
      {
        "type": "folder",
        "name": "reports",
        "children": [
          {"type": "file", "name": "feature_importance.html", "description": "ç‰¹å¾é‡è¦æ€§æŠ¥å‘Š"}
        ]
      }
    ]
  },
  "is_builtin": true,
  "author": "ChainlessChain Team",
  "version": "1.0.0",
  "usage_count": 0,
  "rating": 0,
  "rating_count": 0,
  "created_at": "2024-01-15T10:00:00Z",
  "updated_at": "2024-01-15T10:00:00Z"
}
