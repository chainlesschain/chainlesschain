version: '3.8'

services:
  # Whisper.cpp Local Speech Recognition Service (v0.27.0)
  # 高性能本地语音识别，支持 CPU 和 GPU (CUDA)
  whisper-service:
    build:
      context: ./desktop-app-vue/scripts/whisper-server
      dockerfile: Dockerfile
      args:
        - GPU_SUPPORT=${WHISPER_GPU:-cpu}
    image: chainlesschain/whisper-server:${WHISPER_GPU:-cpu}
    container_name: chainlesschain-whisper
    ports:
      - "8002:8002"
    environment:
      - WHISPER_MODEL_DIR=/app/models
      - WHISPER_MODEL_SIZE=${WHISPER_MODEL:-base}
      - WHISPER_SERVER_PORT=8002
      - WHISPER_DEVICE=${WHISPER_DEVICE:-auto}
      - WHISPER_MAX_FILE_SIZE=52428800
    volumes:
      - whisper-models:/app/models
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8002/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - chainlesschain-network
    # GPU 支持（需要 nvidia-container-toolkit）
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Ollama - 本地LLM服务（CPU版本）
  ollama:
    image: ollama/ollama:latest
    container_name: chainlesschain-ollama
    volumes:
      - ./data/ollama:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODELS=/root/.ollama/models
    restart: unless-stopped
    networks:
      - chainlesschain-network

  # Qdrant - 向量数据库（后端AI服务使用）
  qdrant:
    image: qdrant/qdrant:latest
    container_name: chainlesschain-qdrant
    volumes:
      - ./data/qdrant:/qdrant/storage
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    networks:
      - chainlesschain-network

  # ChromaDB - 向量数据库（Desktop App使用）
  chromadb:
    image: chromadb/chroma:latest
    container_name: chainlesschain-chromadb
    volumes:
      - ./data/chromadb:/chroma/chroma
    ports:
      - "8000:8000"
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=False
    restart: unless-stopped
    networks:
      - chainlesschain-network

  # PostgreSQL - 项目元数据数据库
  postgres:
    image: postgres:16-alpine
    container_name: chainlesschain-postgres
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      # Note: Flyway handles migrations, don't use docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=chainlesschain
      - POSTGRES_USER=chainlesschain
      - POSTGRES_PASSWORD=chainlesschain_pwd_2024
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U chainlesschain -d chainlesschain"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    networks:
      - chainlesschain-network

  # Redis - 缓存和消息队列
  redis:
    image: redis:7-alpine
    container_name: chainlesschain-redis
    volumes:
      - ./data/redis:/data
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --requirepass chainlesschain_redis_2024
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 5s
    restart: unless-stopped
    networks:
      - chainlesschain-network

  # AI服务 - FastAPI Python服务
  ai-service:
    build:
      context: ./backend/ai-service
      dockerfile: Dockerfile
    container_name: chainlesschain-ai-service
    volumes:
      - ./backend/ai-service:/app
      - ./data/projects:/data/projects
    ports:
      - "8001:8000"
    env_file:
      - ./.env  # 使用项目根目录的 .env 文件
    environment:
      # 基础服务连接
      - OLLAMA_HOST=http://ollama:11434
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - REDIS_HOST=redis
      - REDIS_PASSWORD=chainlesschain_redis_2024
      # HuggingFace镜像配置（解决国内网络访问问题）
      - HF_ENDPOINT=https://hf-mirror.com
      - TRANSFORMERS_OFFLINE=0
      # LLM配置 - 默认使用云LLM（dashscope），可在.env中配置API Key
      - LLM_PROVIDER=${LLM_PROVIDER:-dashscope}
      - LLM_MODEL=${LLM_MODEL:-qwen-turbo}
      # 云LLM API Keys（从.env文件读取）
      - DASHSCOPE_API_KEY=${DASHSCOPE_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ZHIPU_API_KEY=${ZHIPU_API_KEY:-}
      - VOLCENGINE_API_KEY=${VOLCENGINE_API_KEY:-}
      - QIANFAN_API_KEY=${QIANFAN_API_KEY:-}
      - HUNYUAN_API_KEY=${HUNYUAN_API_KEY:-}
      - SPARK_API_KEY=${SPARK_API_KEY:-}
      - MINIMAX_API_KEY=${MINIMAX_API_KEY:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
    depends_on:
      ollama:
        condition: service_started
      qdrant:
        condition: service_started
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8000/health\")' || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - chainlesschain-network

  # 项目服务 - Spring Boot服务
  project-service:
    build:
      context: ./backend/project-service
      dockerfile: Dockerfile
    container_name: chainlesschain-project-service
    volumes:
      - ./data/projects:/data/projects
    ports:
      - "9090:9090"
    environment:
      # Spring Boot标准环境变量
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/chainlesschain
      - SPRING_DATASOURCE_USERNAME=chainlesschain
      - SPRING_DATASOURCE_PASSWORD=chainlesschain_pwd_2024
      - SPRING_DATA_REDIS_HOST=redis
      - SPRING_DATA_REDIS_PORT=6379
      - SPRING_DATA_REDIS_PASSWORD=chainlesschain_redis_2024
      # 自定义环境变量（application.yml中使用）
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=chainlesschain
      - DB_USER=chainlesschain
      - DB_PASSWORD=chainlesschain_pwd_2024
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=chainlesschain_redis_2024
      - AI_SERVICE_URL=http://ai-service:8000
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ai-service:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/actuator/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - chainlesschain-network

  # 信令服务器 - WebSocket信令服务（移动端与PC端P2P通讯）
  signaling-server:
    build:
      context: ./signaling-server
      dockerfile: Dockerfile
    container_name: chainlesschain-signaling-server
    ports:
      - "9001:9001"
      - "9002:9002"
    environment:
      - PORT=9001
      - HEALTH_PORT=9002
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9002/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - chainlesschain-network

  # Coturn - STUN/TURN服务器（P2P NAT穿透）
  coturn:
    build:
      context: ./backend/coturn-service
      dockerfile: Dockerfile
    container_name: chainlesschain-coturn
    ports:
      - "3478:3478"
      - "3478:3478/udp"
      - "5349:5349"
      - "5349:5349/udp"
      - "49152-49252:49152-49252/udp"  # TURN relay端口范围（限制为100个端口）
    volumes:
      - ./backend/coturn-service/turnserver.conf:/etc/coturn/turnserver.conf
      - coturn-logs:/var/log/coturn
    environment:
      - DETECT_EXTERNAL_IP=yes
    restart: unless-stopped
    networks:
      - chainlesschain-network

networks:
  chainlesschain-network:
    driver: bridge

volumes:
  ollama-data:
  qdrant-data:
  postgres-data:
  redis-data:
  whisper-models:
  coturn-logs:
