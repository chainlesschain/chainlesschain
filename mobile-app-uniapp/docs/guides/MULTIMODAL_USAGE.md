# å¤šæ¨¡æ€åŠŸèƒ½ä½¿ç”¨æŒ‡å—

## ğŸ“– æ¦‚è¿°

ç§»åŠ¨ç«¯å¤šæ¨¡æ€ç®¡ç†å™¨æ”¯æŒå›¾åƒ+æ–‡æœ¬æ··åˆè¾“å…¥ï¼Œå¯ä»¥ä½¿ç”¨GPT-4Vã€Claude 3ã€Qwen-VLç­‰è§†è§‰æ¨¡å‹è¿›è¡Œå›¾åƒç†è§£ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬é…ç½®

```javascript
import { getMultimodalManager } from '@/services/llm/multimodal-manager'

const multimodal = getMultimodalManager({
  // APIå¯†é’¥
  openaiApiKey: 'sk-...',
  anthropicApiKey: 'sk-ant-...',
  dashscopeApiKey: 'sk-...',

  // é»˜è®¤æ¨¡å‹
  defaultModel: 'gpt-4-vision-preview',

  // å›¾åƒå¤„ç†é…ç½®
  maxImageSize: 2048,
  maxFileSize: 5 * 1024 * 1024,
  imageQuality: 0.8,

  // ç¼“å­˜é…ç½®
  enableCache: true,
  cacheSize: 50
})

// åˆå§‹åŒ–
await multimodal.initialize()
```

### 2. å›¾åƒé—®ç­”

```javascript
// å•å¼ å›¾åƒ
const result = await multimodal.askAboutImage(
  '/path/to/image.jpg',
  'è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ'
)

console.log(result.content) // AIçš„å›ç­”

// å¤šå¼ å›¾åƒ
const result2 = await multimodal.askAboutImage(
  ['/path/to/image1.jpg', '/path/to/image2.jpg'],
  'æ¯”è¾ƒè¿™ä¸¤å¼ å›¾ç‰‡çš„å¼‚åŒ'
)
```

### 3. å›¾åƒæè¿°

```javascript
const result = await multimodal.describeImage('/path/to/image.jpg')

console.log(result.content) // è¯¦ç»†çš„å›¾åƒæè¿°
```

### 4. å›¾åƒOCR

```javascript
const result = await multimodal.extractTextFromImage('/path/to/document.jpg')

console.log(result.content) // æå–çš„æ–‡å­—
```

## ğŸ“‹ å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

### åœºæ™¯1: åœ¨ç¬”è®°ä¸­ä½¿ç”¨å›¾åƒç†è§£

```vue
<template>
  <view class="image-note">
    <image :src="imagePath" mode="aspectFit" />

    <button @click="analyzeImage">åˆ†æå›¾ç‰‡</button>

    <view v-if="analysis" class="analysis">
      {{ analysis }}
    </view>
  </view>
</template>

<script>
import { getMultimodalManager } from '@/services/llm/multimodal-manager'

export default {
  data() {
    return {
      imagePath: '',
      analysis: ''
    }
  },

  methods: {
    async analyzeImage() {
      const multimodal = getMultimodalManager()

      uni.showLoading({ title: 'åˆ†æä¸­...' })

      const result = await multimodal.describeImage(this.imagePath, {
        model: 'gpt-4-vision-preview'
      })

      uni.hideLoading()

      if (result.success) {
        this.analysis = result.content
      } else {
        uni.showToast({
          title: 'åˆ†æå¤±è´¥',
          icon: 'none'
        })
      }
    }
  }
}
</script>
```

### åœºæ™¯2: å›¾åƒé—®ç­”å¯¹è¯

```javascript
import { getMultimodalManager } from '@/services/llm/multimodal-manager'

export default {
  data() {
    return {
      messages: [],
      currentImage: ''
    }
  },

  methods: {
    async sendMessage(text) {
      const multimodal = getMultimodalManager()

      // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
      const userMessage = {
        role: 'user',
        content: text
      }

      // å¦‚æœæœ‰å›¾åƒï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
      if (this.currentImage) {
        userMessage.images = [this.currentImage]
      }

      this.messages.push(userMessage)

      // è°ƒç”¨API
      const result = await multimodal.chat(this.messages, {
        model: 'gpt-4-vision-preview'
      })

      if (result.success) {
        // æ·»åŠ AIå›å¤
        this.messages.push({
          role: 'assistant',
          content: result.content
        })
      }
    },

    selectImage() {
      uni.chooseImage({
        count: 1,
        success: (res) => {
          this.currentImage = res.tempFilePaths[0]
        }
      })
    }
  }
}
```

### åœºæ™¯3: æ‰¹é‡å›¾åƒåˆ†æ

```javascript
async function batchAnalyzeImages(imagePaths) {
  const multimodal = getMultimodalManager()

  const results = []

  for (const imagePath of imagePaths) {
    const result = await multimodal.analyzeImage(imagePath, 'å†…å®¹', {
      model: 'claude-3-haiku' // ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
    })

    if (result.success) {
      results.push({
        path: imagePath,
        analysis: result.content
      })
    }
  }

  return results
}
```

### åœºæ™¯4: å›¾åƒä¿¡æ¯æå–

```javascript
async function extractProductInfo(productImagePath) {
  const multimodal = getMultimodalManager()

  const result = await multimodal.askAboutImage(
    productImagePath,
    `è¯·ä»è¿™å¼ äº§å“å›¾ç‰‡ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
    1. äº§å“åç§°
    2. å“ç‰Œ
    3. ä¸»è¦ç‰¹å¾
    4. ä»·æ ¼ï¼ˆå¦‚æœå¯è§ï¼‰
    è¯·ä»¥JSONæ ¼å¼è¿”å›ã€‚`,
    {
      model: 'gpt-4-vision-preview',
      maxTokens: 500
    }
  )

  if (result.success) {
    try {
      return JSON.parse(result.content)
    } catch (e) {
      return { raw: result.content }
    }
  }

  return null
}
```

## ğŸ¯ æ”¯æŒçš„æ¨¡å‹

### OpenAI GPT-4V

```javascript
const result = await multimodal.chat(messages, {
  model: 'gpt-4-vision-preview',  // æˆ– 'gpt-4o'
  maxTokens: 1000,
  temperature: 0.7
})
```

**ç‰¹ç‚¹**:
- å¼ºå¤§çš„å›¾åƒç†è§£èƒ½åŠ›
- æ”¯æŒå¤šå›¾åƒæ¯”è¾ƒ
- æ”¯æŒURLå’Œbase64å›¾åƒ

### Anthropic Claude 3

```javascript
const result = await multimodal.chat(messages, {
  model: 'claude-3-sonnet',  // æˆ– 'claude-3-opus', 'claude-3-haiku'
  maxTokens: 1000,
  temperature: 0.7
})
```

**ç‰¹ç‚¹**:
- ç»†è‡´çš„å›¾åƒåˆ†æ
- è¾ƒå¥½çš„OCRèƒ½åŠ›
- ä»…æ”¯æŒbase64å›¾åƒ

### Alibaba Qwen-VL

```javascript
const result = await multimodal.chat(messages, {
  model: 'qwen-vl-plus',  // æˆ– 'qwen-vl-max'
  maxTokens: 1000,
  temperature: 0.7
})
```

**ç‰¹ç‚¹**:
- ä¸­æ–‡ç†è§£ä¼˜ç§€
- æ”¯æŒURLå’Œbase64
- æ€§ä»·æ¯”é«˜

## ğŸ“¸ å›¾åƒè¾“å…¥æ ¼å¼

### 1. æœ¬åœ°æ–‡ä»¶è·¯å¾„

```javascript
const result = await multimodal.askAboutImage(
  '/path/to/local/image.jpg',
  'æè¿°è¿™å¼ å›¾ç‰‡'
)
```

### 2. URL

```javascript
const result = await multimodal.askAboutImage(
  'https://example.com/image.jpg',
  'æè¿°è¿™å¼ å›¾ç‰‡'
)
```

### 3. Base64

```javascript
const result = await multimodal.askAboutImage(
  'data:image/jpeg;base64,/9j/4AAQSkZJRg...',
  'æè¿°è¿™å¼ å›¾ç‰‡'
)
```

### 4. uni.chooseImageè·å–çš„å›¾åƒ

```javascript
uni.chooseImage({
  count: 1,
  success: async (res) => {
    const imagePath = res.tempFilePaths[0]

    const result = await multimodal.askAboutImage(
      imagePath,
      'è¿™æ˜¯ä»€ä¹ˆï¼Ÿ'
    )

    console.log(result.content)
  }
})
```

## ğŸ› ï¸ ä¾¿æ·æ–¹æ³•

### askAboutImage - å›¾åƒé—®ç­”

```javascript
await multimodal.askAboutImage(images, question, options)
```

### describeImage - å›¾åƒæè¿°

```javascript
await multimodal.describeImage(images, options)
```

### extractTextFromImage - å›¾åƒOCR

```javascript
await multimodal.extractTextFromImage(images, options)
```

### analyzeImage - å›¾åƒåˆ†æ

```javascript
await multimodal.analyzeImage(images, aspect, options)
```

**aspectå¯é€‰å€¼**:
- `'å†…å®¹'` - åˆ†æå›¾åƒæ•´ä½“å†…å®¹
- `'æƒ…æ„Ÿ'` - åˆ†æå›¾åƒä¼ è¾¾çš„æƒ…æ„Ÿ
- `'ç‰©ä½“'` - è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“
- `'åœºæ™¯'` - åˆ†æå›¾åƒåœºæ™¯
- `'é¢œè‰²'` - åˆ†æå›¾åƒè‰²å½©
- è‡ªå®šä¹‰

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å¯ç”¨ç¼“å­˜

```javascript
const multimodal = getMultimodalManager({
  enableCache: true,
  cacheSize: 50
})
```

ç›¸åŒå›¾åƒçš„é‡å¤è¯·æ±‚ä¼šä»ç¼“å­˜ä¸­è·å–ï¼Œå¤§å¹…æå‡é€Ÿåº¦ã€‚

### 2. å›¾åƒå‹ç¼©

```javascript
const multimodal = getMultimodalManager({
  maxImageSize: 1024,     // æœ€å¤§å°ºå¯¸
  imageQuality: 0.7,      // å‹ç¼©è´¨é‡
  maxFileSize: 2 * 1024 * 1024  // æœ€å¤§æ–‡ä»¶å¤§å°
})
```

### 3. é€‰æ‹©åˆé€‚çš„æ¨¡å‹

- **å¿«é€Ÿå“åº”**: `claude-3-haiku`, `qwen-vl-plus`
- **é«˜è´¨é‡**: `gpt-4-vision-preview`, `claude-3-opus`
- **å¹³è¡¡**: `claude-3-sonnet`, `qwen-vl-max`

## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯

```javascript
const stats = multimodal.getStats()

console.log(stats)
// {
//   totalRequests: 10,
//   successfulRequests: 9,
//   failedRequests: 1,
//   successRate: '90.00%',
//   imagesProcessed: 15,
//   cacheHits: 5,
//   cacheMisses: 10,
//   cacheHitRate: '33.33%',
//   cacheSize: 10
// }
```

## ğŸ¨ æœ€ä½³å®è·µ

### 1. æä¾›æ¸…æ™°çš„é—®é¢˜

âŒ å·®çš„é—®é¢˜:
```javascript
await multimodal.askAboutImage(image, 'è¿™æ˜¯å•¥ï¼Ÿ')
```

âœ… å¥½çš„é—®é¢˜:
```javascript
await multimodal.askAboutImage(
  image,
  'è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„ä¸»è¦ç‰©ä½“ã€åœºæ™¯å’Œè‰²å½©ã€‚'
)
```

### 2. ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º

```javascript
const result = await multimodal.askAboutImage(
  image,
  `è¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œä»¥JSONæ ¼å¼è¿”å›:
  {
    "objects": ["ç‰©ä½“åˆ—è¡¨"],
    "scene": "åœºæ™¯æè¿°",
    "mood": "æƒ…æ„ŸåŸºè°ƒ",
    "colors": ["ä¸»è¦é¢œè‰²"]
  }`
)

const data = JSON.parse(result.content)
```

### 3. å¤„ç†å¤±è´¥æƒ…å†µ

```javascript
const result = await multimodal.askAboutImage(image, question)

if (!result.success) {
  console.error('åˆ†æå¤±è´¥:', result.error)

  // é™çº§å¤„ç†
  uni.showToast({
    title: 'å›¾åƒåˆ†ææš‚æ—¶ä¸å¯ç”¨',
    icon: 'none'
  })
}
```

### 4. æ‰¹å¤„ç†æ—¶æ§åˆ¶é€Ÿç‡

```javascript
async function batchProcess(images) {
  const results = []

  for (const image of images) {
    const result = await multimodal.describeImage(image)
    results.push(result)

    // é¿å…è¿‡å¿«è¯·æ±‚
    await new Promise(resolve => setTimeout(resolve, 1000))
  }

  return results
}
```

## ğŸ”§ é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯

1. **APIå¯†é’¥æœªé…ç½®**
   ```
   é”™è¯¯: æœªé…ç½®APIå¯†é’¥ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨
   è§£å†³: é…ç½®å¯¹åº”æ¨¡å‹çš„APIå¯†é’¥
   ```

2. **å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨**
   ```
   é”™è¯¯: è¯»å–å›¾åƒå¤±è´¥: file not found
   è§£å†³: æ£€æŸ¥å›¾åƒè·¯å¾„æ˜¯å¦æ­£ç¡®
   ```

3. **å›¾åƒæ–‡ä»¶è¿‡å¤§**
   ```
   é”™è¯¯: å›¾åƒæ–‡ä»¶è¿‡å¤§ï¼Œå¯èƒ½éœ€è¦å‹ç¼©
   è§£å†³: é™ä½maxFileSizeæˆ–å‹ç¼©å›¾åƒ
   ```

4. **æ¨¡å‹ä¸æ”¯æŒ**
   ```
   é”™è¯¯: ä¸æ”¯æŒçš„æ¨¡å‹: xxx
   è§£å†³: ä½¿ç”¨getSupportedModels()æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹
   ```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **APIè´¹ç”¨**: å¤šæ¨¡æ€APIé€šå¸¸æ¯”çº¯æ–‡æœ¬æ›´è´µï¼Œæ³¨æ„æ§åˆ¶ä½¿ç”¨é‡
2. **éšç§**: ä¸Šä¼ åˆ°APIçš„å›¾åƒä¼šè¢«ç¬¬ä¸‰æ–¹å¤„ç†ï¼Œæ³¨æ„éšç§é—®é¢˜
3. **å›¾åƒå¤§å°**: è¿‡å¤§çš„å›¾åƒä¼šå¢åŠ å¤„ç†æ—¶é—´å’Œè´¹ç”¨
4. **ç¼“å­˜**: å¯ç”¨ç¼“å­˜å¯ä»¥èŠ‚çœè´¹ç”¨å’Œæå‡é€Ÿåº¦
5. **ç½‘ç»œ**: å›¾åƒä¸Šä¼ éœ€è¦è‰¯å¥½çš„ç½‘ç»œè¿æ¥

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [å¤šæ¨¡æ€ç®¡ç†å™¨APIæ–‡æ¡£](./API.md#multimodal)
- [LLMç®¡ç†å™¨æ–‡æ¡£](./LLM_MANAGER.md)
- [ç§»åŠ¨ç«¯ä¼˜åŒ–æŠ¥å‘Š](../MOBILE_OPTIMIZATION_REPORT.md)
